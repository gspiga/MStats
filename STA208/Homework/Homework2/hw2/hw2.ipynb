{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2ba0a96",
   "metadata": {},
   "source": [
    "# STA 208: Homework 2 (Do not distribute)\n",
    "\n",
    "## Due 05/12/2023 midnight (11:59pm)\n",
    "\n",
    "__Instructions:__ \n",
    "\n",
    "1. Submit your homework using one file name ”LastName_FirstName_hw2.html” on canvas. \n",
    "2. The written portions can be either done in markdown and TeX in new cells or written by hand and scanned. Using TeX is strongly preferred. However, if you have scanned solutions for handwriting, you can submit a zip file. Please make sure your handwriting is clear and readable and your scanned files are displayed properly in your jupyter notebook. \n",
    "3. Your code should be readable; writing a piece of code should be compared to writing a page of a book. Adopt the one-statement-per-line rule. Consider splitting a lengthy statement into multiple lines to improve readability. (You will lose one point for each line that does not follow the one-statementper-line rule)\n",
    "4. To help understand and maintain code, you should always add comments to explain your code. (homework with no comments will receive 0 points). For a very long comment, please break it into multiple lines.\n",
    "5. In your Jupyter Notebook, put your answers in new cells after each exercise. You can make as many new cells as you like. Use code cells for code and Markdown cells for text.\n",
    "6. Please make sure to print out the necessary results to avoid losing points. We should not run your code to figure out your answers. \n",
    "7. However, also make sure we are able to open this notebook and run everything here by running the cells in sequence; in case that the TA wants to check the details.\n",
    "8. You will be graded on correctness of your math, code efficiency and succinctness, and conclusions and modelling decisions\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6525107",
   "metadata": {},
   "source": [
    "### Exercise 1 (Logistic regression)\n",
    "\n",
    "(15 points) In class, we studied the logit model with 2 classes. Now consider the multilogit model with $K$ classes. Let $\\beta$ be the $(p+1)(K-1)$-vector consisting of all the coefficients. Define a suitably enlarged version of the input vector x to accomodate this vectorized coefficient matrix. Derive the Newton-Raphson algorithm for maximizing the multinomial log-likelihood, and describe how you implement the algorithm (e.g., you can write a sudo code). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "df9a0ff5",
   "metadata": {},
   "source": [
    "$$\n",
    "x, y \\in {0,1} \\\\\n",
    "P(Y = y | X) = P(Y = 1 | X )^y P(Y= 0|x )^{1 - y}\\\\ \n",
    "log(P(Y=y|x)) = y log(P(Y=1|x)) + (1-y)log(P(Y=0|x)) \\\\\n",
    "= y log \\frac{P(Y=1|x)} {P(Y = 0|X )} + log(P(Y=0|X)) \\\\\n",
    "\\\\\n",
    "log \\frac{P(Y=1|X)}{P(Y = 0|X)} = \\beta_0 + \\beta_1X_1 + \\ldots \\\\ \n",
    "log \\frac{P(Y=1|X)}{P(Y = 0|X)} = X^T\\beta \\\\\n",
    "P(Y = 1|X) = \\frac{e^{x^T\\beta}}{1 + e^{x^T\\beta}} \\\\\n",
    "P(Y = 0|X) = \\frac{1}{1 + e^{x^T\\beta}} \\\\\n",
    "\\\\\n",
    "\\beta = \\begin{bmatrix}\n",
    "        \\beta_1 \\\\\n",
    "        \\beta_2 \\\\\n",
    "        \\vdots \\\\\n",
    "        \\beta_{p}\n",
    "        \\end{bmatrix}\n",
    "\n",
    "X = \\begin{bmatrix}\n",
    "        1 \\\\\n",
    "        x_1 \\\\\n",
    "        x_2 \\\\\n",
    "        \\vdots \\\\\n",
    "        x_{p}\n",
    "        \\end{bmatrix} \\\\\n",
    "log P(Y= y |x ) = y*x^T\\beta + log P(Y = 0|X) \\\\\n",
    " = y*x^T\\beta - log (1 + e^{x^T\\beta}) \\\\\n",
    "\n",
    " X_i, y_i (i = ,1 \\ldots ,n): \\\\\n",
    " \\sum_{i =1}^n log P(y_i|x_i) =\\sum_{i =1}^n y_i*x_i^T\\beta - log (1 + e^{x_i^T\\beta}) \n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a99a870",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f02eaca",
   "metadata": {},
   "source": [
    "### Exercise 2 (Support vector machine)\n",
    "\n",
    "_Natural language processing_ (NLP) is a branch of artificial intelligence which gives computers the ability to learn text and spoken words in much the same way human beings can.\n",
    "\n",
    "In python, text data can be converted into vector data through a vectorization operation.\n",
    "Two vectorizer packages in Python are ``sklearn.feature_extraction.text.CountVectorizer`` and ``sklearn.feature_extraction.text.TfidfVectorizer``. A corpus is a collection of documents and the dictionary is all of the words in the corpus. A simple vectorizer will let $X_{i,j}$ be the number of times the $j$th word is in the $i$th document. \n",
    "\n",
    "Bag-of-words models is one of the most popular model in NLP. The model treats each document as a set of words but ignoring the order of those words. \n",
    "\n",
    "In this exercise, you will learn how to classify a text using SVM. The dataset includes two CSV files (`Corona_NLP_train.csv` and `Corona_NLP_test.csv`) that contain IDs and sentiment scores of the tweets related to the COVID-19 pandemic. The real-time Twitter feed is monitored for coronavirus-related tweets using 90+ different keywords and hashtags that are commonly used while referencing the pandemic. The oldest tweets in this dataset date back to October 01, 2019. \n",
    "\n",
    "\n",
    "The training dataset contains five columns: \n",
    "- UserName\t\n",
    "- ScreenName\t\n",
    "- Location\t\n",
    "- TweetAt\t\n",
    "- OriginalTweet\t\n",
    "- Sentiment (five labels: `extremely positive`, `positive`, `negative`, `extremely negative`, `neutral`)\n",
    "\n",
    "The task is to predict sentiment basedon the original tweet. Here, we combine `extremely positive` and `positive` to `positive` and combine `extremely negative` and `negative` to `negative`. So, the sentiment contains three labels.\n",
    "Your goal is to apply svm to predict the three labels based on OriginalTweet. Indeed, one can view this as a classification problem with three labels. \n",
    "\n",
    "I already attached the file `dataprocessing.ipynb` for processing the data. The code is directly copied from this [website](https://www.kaggle.com/code/mehmetlaudatekman/text-classification-svm-explained/notebook).\n",
    "\n",
    "Please answer the following questions:\n",
    " \n",
    "1. (15 points) Use sklearn svm.SVC on the TRAIN split (`Corona_NLP_train.csv`) and predict on the TEST split (`Corona_NLP_test.csv`). Plot your ROC and PR (Precision-Recall) curves for predicting `positive` (versus everything else); use the linear kernel and set the C parameter to be 1. Do the same for predicting the `negative` label versus everything else. Please write the code for generating the ROC curve by yourself.\n",
    "2. (10 points) In this problem, we have three labels (instead of two). In class, we only learned SVM for solving a two-class classification problem. Describe (using your own words) how the python package `svm.SVC` fits SVM for multi-class classification.\n",
    "3. (10 points) Choose several different values for $C$ (some are smaller than 1, some are bigger than 1), plot the ROC curves for predicting 'positive' (versus everything else), and predicting 'negative' (versus everything else). Comment on your findings. \n",
    "4. (Bonus 10 points) Explore how to use logistic regression to classify this text. Implement the method. Comment on its prediction accuracy and compare the ROC curve with the SVM ROC curve. \n",
    "\n",
    "__Note:__ the PR curve is a ratio of the number of true positives divided by the sum of the true positives and false positives. It describes how good a model is at predicting the positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0d09acc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle\n",
    "import time\n",
    "import re\n",
    "from IPython.display import display "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6831340a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       OriginalTweet           Sentiment\n",
       "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral\n",
       "1  advice Talk to your neighbours family to excha...            Positive\n",
       "2  Coronavirus Australia: Woolworths to give elde...            Positive\n",
       "3  My food stock is not the only one which is emp...            Positive\n",
       "4  Me, ready to go at supermarket during the #COV...  Extremely Negative"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Positive              11422\n",
       "Negative               9917\n",
       "Neutral                7713\n",
       "Extremely Positive     6624\n",
       "Extremely Negative     5481\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive              11422\n",
      "Extremely Positive     6624\n",
      "Name: Sentiment, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>As news of the regionÂs first confirmed COVID...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Cashier at grocery store was sharing his insig...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       OriginalTweet Sentiment\n",
       "1  advice Talk to your neighbours family to excha...  Positive\n",
       "2  Coronavirus Australia: Woolworths to give elde...  Positive\n",
       "3  My food stock is not the only one which is emp...  Positive\n",
       "5  As news of the regionÂs first confirmed COVID...  Positive\n",
       "6  Cashier at grocery store was sharing his insig...  Positive"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative              9917\n",
      "Extremely Negative    5481\n",
      "Name: Sentiment, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>For corona prevention,we should stop to buy th...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>with 100  nations inficted with  covid  19  th...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>@10DowningStreet @grantshapps what is being do...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>In preparation for higher demand and a potenti...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        OriginalTweet           Sentiment\n",
       "4   Me, ready to go at supermarket during the #COV...  Extremely Negative\n",
       "9   For corona prevention,we should stop to buy th...            Negative\n",
       "20  with 100  nations inficted with  covid  19  th...  Extremely Negative\n",
       "24  @10DowningStreet @grantshapps what is being do...            Negative\n",
       "26  In preparation for higher demand and a potenti...            Negative"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutral    7713\n",
      "Name: Sentiment, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Was at the supermarket today. Didn't buy toile...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>All month there hasn't been crowding in the su...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>????? ????? ????? ????? ??\\r\\r\\n?????? ????? ?...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>@eyeonthearctic 16MAR20 Russia consumer survei...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        OriginalTweet Sentiment\n",
       "0   @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...   Neutral\n",
       "7   Was at the supermarket today. Didn't buy toile...   Neutral\n",
       "10  All month there hasn't been crowding in the su...   Neutral\n",
       "16  ????? ????? ????? ????? ??\\r\\r\\n?????? ????? ?...   Neutral\n",
       "17  @eyeonthearctic 16MAR20 Russia consumer survei...   Neutral"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 44955 entries, 0 to 44954\n",
      "Data columns (total 3 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   index          44955 non-null  int64 \n",
      " 1   OriginalTweet  44955 non-null  object\n",
      " 2   Sentiment      44955 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 1.0+ MB\n",
      "None\n",
      "   index                                      OriginalTweet  Sentiment\n",
      "0      1  advice Talk to your neighbours family to excha...          2\n",
      "1      2  Coronavirus Australia: Woolworths to give elde...          2\n",
      "2      3  My food stock is not the only one which is emp...          2\n",
      "3      5  As news of the regionÂs first confirmed COVID...          2\n",
      "4      6  Cashier at grocery store was sharing his insig...          2\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing \n",
    "\"\"\" \n",
    "# Loading in the data\n",
    "trainSet =pd.read_csv(\"Corona_NLP_train.csv\", encoding=\"latin1\")\n",
    "testSet = pd.read_csv(\"Corona_NLP_test.csv\", encoding = \"latin1\")\n",
    "\n",
    "unrelevant_features = [\"UserName\", \"ScreenName\", \"Location\", \"TweetAt\"]\n",
    "trainSet.drop(unrelevant_features, inplace = True, axis = 1)\n",
    "testSet.drop(unrelevant_features, inplace = True, axis = 1)\n",
    "#display(trainSet.head())\n",
    "\n",
    "trainSet.Sentiment = trainSet.Sentiment.replace(\"Extremely Postive\", \"Positive\")\n",
    "trainSet.Sentiment = trainSet.Sentiment.replace(\"Extremely Negative\", \"Negative\")\n",
    "#display(trainSet.head())\n",
    "\n",
    "testSet.Sentiment = testSet.Sentiment.replace(\"Extremely Positive\", \"Positive\")\n",
    "testSet.Sentiment = testSet.Sentiment.replace(\"Extremely Negative\", \"Negative\")\n",
    "\n",
    "# Convert negatives as 0, neutrals as 1, positives as 2, \n",
    "mapping = {\"Negative\": 0, \"Neutral\": 1, \"Positive\":2}\n",
    "trainSet.Sentiment = trainSet.Sentiment.replace(mapping)\n",
    "testSet.Sentiment = testSet.Sentiment.replace(mapping)\n",
    "\n",
    "data = pd.concat([trainSet, testSet])\n",
    "display(data.info())\n",
    "display(data.head()) \"\"\"\n",
    "\n",
    "train_set = pd.read_csv('Corona_NLP_train.csv',encoding=\"latin1\") # do not forget to change the path\n",
    "test_set = pd.read_csv('Corona_NLP_test.csv',encoding=\"latin1\")\n",
    "\n",
    "# remove unrelevant_features\n",
    "\n",
    "unrelevant_features = [\"UserName\",\"ScreenName\",\"Location\",\"TweetAt\"]\n",
    "train_set.drop(unrelevant_features,inplace=True,axis=1)\n",
    "test_set.drop(unrelevant_features,inplace=True,axis=1)\n",
    "display(train_set.head())\n",
    "\n",
    "# split data based on sentiment values: positive, neutral or negative.\n",
    "# Extremely positive is combined with positive. Similar to extremely negative\n",
    "display(train_set[\"Sentiment\"].value_counts())\n",
    "\n",
    "positives = train_set[(train_set[\"Sentiment\"] == \"Positive\") | (train_set[\"Sentiment\"] == \"Extremely Positive\")]\n",
    "positives_test = test_set[(test_set[\"Sentiment\"] == \"Positive\") | (test_set[\"Sentiment\"] == \"Extremely Positive\")]\n",
    "print(positives[\"Sentiment\"].value_counts())\n",
    "display(positives.head())\n",
    "\n",
    "negatives = train_set[(train_set[\"Sentiment\"] == \"Negative\") | (train_set[\"Sentiment\"] == \"Extremely Negative\")]\n",
    "negatives_test = test_set[(test_set[\"Sentiment\"] == \"Negative\") | (test_set[\"Sentiment\"] == \"Extremely Negative\")]\n",
    "print(negatives[\"Sentiment\"].value_counts())\n",
    "display(negatives.head())\n",
    "\n",
    "neutrals = train_set[train_set[\"Sentiment\"] == \"Neutral\"]\n",
    "neutrals_test = test_set[test_set[\"Sentiment\"] == \"Neutral\"]\n",
    "print(neutrals[\"Sentiment\"].value_counts())\n",
    "display(neutrals.head())\n",
    "\n",
    "# Convert labels into integers \n",
    "# convert negatives as 0\n",
    "# neutrals as 1 \n",
    "# and positives as 2.\n",
    "\n",
    "import warnings as wrn\n",
    "wrn.filterwarnings('ignore')\n",
    "\n",
    "negatives[\"Sentiment\"] = 0 \n",
    "negatives_test[\"Sentiment\"] = 0\n",
    "\n",
    "positives[\"Sentiment\"] = 2\n",
    "positives_test[\"Sentiment\"] = 2\n",
    "\n",
    "neutrals[\"Sentiment\"] = 1\n",
    "neutrals_test[\"Sentiment\"] = 1\n",
    "\n",
    "\n",
    "# concatenate train and test first, will split them after processing.\n",
    "\n",
    "data = pd.concat([positives,\n",
    "                  positives_test,\n",
    "                  neutrals,\n",
    "                  neutrals_test,\n",
    "                  negatives,\n",
    "                  negatives_test\n",
    "                 ],axis=0)\n",
    "\n",
    "data.reset_index(inplace=True)\n",
    "\n",
    "print(data.info())\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9853800d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('omw-1.4')\n",
    "\n",
    "cleanedData = []\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "swords = stopwords.words(\"english\")\n",
    "for text in data[\"OriginalTweet\"]:\n",
    "    \n",
    "    # Cleaning links\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # Cleaning everything except alphabetical and numerical characters\n",
    "    text = re.sub(\"[^a-zA-Z0-9]\",\" \",text)\n",
    "    \n",
    "    # Tokenizing and lemmatizing\n",
    "    text = nltk.word_tokenize(text.lower())\n",
    "    text = [lemma.lemmatize(word) for word in text]\n",
    "    \n",
    "    # Removing stopwords\n",
    "    text = [word for word in text if word not in swords]\n",
    "    \n",
    "    # Joining\n",
    "    text = \" \".join(text)\n",
    "    \n",
    "    cleanedData.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fb0e96be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "advice talk neighbour family exchange phone number create contact list phone number neighbour school employer chemist gp set online shopping account po adequate supply regular med order\n",
      "\n",
      "coronavirus australia woolworth give elderly disabled dedicated shopping hour amid covid 19 outbreak\n",
      "\n",
      "food stock one empty please panic enough food everyone take need stay calm stay safe covid19france covid 19 covid19 coronavirus confinement confinementotal confinementgeneral\n",
      "\n",
      "news region first confirmed covid 19 case came sullivan county last week people flocked area store purchase cleaning supply hand sanitizer food toilet paper good tim dodson report\n",
      "\n",
      "cashier grocery store wa sharing insight covid 19 prove credibility commented civics class know talking\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the output text\n",
    "\n",
    "for i in range(0,5):\n",
    "    print(cleanedData[i],end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "31428287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the bag of words\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=5)\n",
    "BOW = vectorizer.fit_transform(cleanedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4b985b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33716, 5)\n",
      "(11239, 5)\n",
      "(33716,)\n",
      "(11239,)\n"
     ]
    }
   ],
   "source": [
    "# split the dataset into training and test\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(BOW,np.asarray(data[\"Sentiment\"]))\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "67fa7ee5",
   "metadata": {},
   "source": [
    "#### 1.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3ba7e732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting SVC took 173.52 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "start_time = time.time()\n",
    "\n",
    "model = SVC(C = 1, kernel=\"linear\", probability=True)\n",
    "model.fit(x_train,y_train)\n",
    "\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "process_time = round(end_time-start_time,2)\n",
    "print(\"Fitting SVC took {} seconds\".format(process_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb1c482",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.predict(x_test)\n",
    "\n",
    "# Generate data that is all positive\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "519aba15",
   "metadata": {},
   "source": [
    "### Exercise 3 (K-means and PCA)\n",
    "\n",
    "Load the poses.csv dataset, which is a concatenation of other datasets to form a larger dataset. The task column in the dataset contains six poses: sitting, lying, walking, standing, cycling, bending. I want you to act like the dataset is from the same experiment. You need to open the file and take a look the dataset first. Combining bending1 and bending2 together. \n",
    "\n",
    "1. (15 pts) Apply 1 time lag difference of the dataset, so that each variable is the difference of the time point and the previous time point.  Standardize the dataset and remove any variables that do not make sense.  Run the PCA decomposition with 2 principal components.  Plot the 2 principal components.  Which variables have the most loading on the principal components (look at `.components_`)?\n",
    "\n",
    "1. (15 pts) Also on the 1 lagged dataset.  Run K-means clustering (with 6 clusters), how much does the cluster overlap with the 'task' variable.  Look at the confusion matrix (`sklearn.metrics.confusion_matrix`) of the cluster against the 'task'.  Is there a clear mapping from clusters to task?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
