{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96dd0c0e",
   "metadata": {},
   "source": [
    "# STA 208: Homework 1 (Do not distribute)\n",
    "\n",
    "## Due 04/28/2023 midnight (11:59pm)\n",
    "\n",
    "__Instructions:__ \n",
    "\n",
    "1. Submit your homework using one file name ”LastName_FirstName hw1.html” on canvas. \n",
    "2. The written portions can be either done in markdown and TeX in new cells or written by hand and scanned. Using TeX is strongly preferred. However, if you have scanned solutions for handwriting, you can submit a zip file. Please make sure your handwriting is clear and readable and your scanned files are displayed properly in your jupyter notebook. \n",
    "3. Your code should be readable; writing a piece of code should be compared to writing a page of a book. Adopt the one-statement-per-line rule. Consider splitting a lengthy statement into multiple lines to improve readability. (You will lose one point for each line that does not follow the one-statementper-line rule)\n",
    "4. To help understand and maintain code, you should always add comments to explain your code. (homework with no comments will receive 0 points). For a very long comment, please break it into multiple lines.\n",
    "5. In your Jupyter Notebook, put your answers in new cells after each exercise. You can make as many new cells as you like. Use code cells for code and Markdown cells for text.\n",
    "6. Please make sure to print out the necessary results to avoid losing points. We should not run your code to figure out your answers. \n",
    "7. However, also make sure we are able to open this notebook and run everything here by running the cells in sequence; in case that the TA wants to check the details.\n",
    "8. You will be graded on correctness of your math, code efficiency and succinctness, and conclusions and modelling decisions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43772f97",
   "metadata": {},
   "source": [
    "### Exercise 1 (Empirical risk minimization) (20 pts, 5 pts each)\n",
    "\n",
    "Consider Poisson model with rate parameter $\\lambda$ which has PMF,\n",
    "$$\n",
    "p(y|\\lambda) = \\frac{\\lambda^y}{y!} e^{-\\lambda},\n",
    "$$\n",
    "where $y = 0,1,\\ldots$ is some count variable.\n",
    "In Poison regression, we model $\\lambda = e^{\\beta^\\top x}$ to obtain $p(y | x,\\beta)$.\n",
    "\n",
    "1. Let the loss function for Poisson regression be $\\ell_i(\\beta) = - \\log p(y_i | x_i, \\beta)$ for a dataset consisting of predictor variables and count values $\\{x_i,y_i\\}_{i=1}^n$.  Here $\\propto$ means that we disregard any additive terms that are not dependent on $\\beta$.  Write an expression for $\\ell_i$ and derive its gradient. \n",
    "2. Show that the empirical risk $R_n(\\beta)$ is a convex function of $\\beta$.\n",
    "3. Consider the mapping $F_\\eta(\\beta) = \\beta - \\eta \\nabla R_n(\\beta)$ which is the iteration of gradient descent ($\\eta>0$ is called the learning parameter).  Show that at the minimizer of $R_n$, $\\hat \\beta$, we have that $F(\\hat \\beta) = \\hat \\beta$.\n",
    "4. I have a script to simulate from this model below.  Implement the gradient descent algorithm above and show that with enough data ($n$ large enough) the estimated $\\hat \\beta$ approaches the true $\\beta$ (you can look at the sum of square error between these two vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3ad3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb9d5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simulate from the Poisson regression model (use y,X)\n",
    "np.random.seed(2022)\n",
    "n, p = 1000,20\n",
    "X = np.random.normal(0,1,size = (n,p))\n",
    "beta = np.random.normal(0,.2,size = (p))\n",
    "lamb = np.exp(X @ beta)\n",
    "y = np.random.poisson(lamb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "360cc371",
   "metadata": {},
   "source": [
    "### Exercise 2 (Regression and OLS) (35 pts, 5 pts each)\n",
    "\n",
    "Consider the regression setting in which $x_i \\in \\mathbb R^p$ and $y_i \\in \\mathbb R$, for $i=1,\\ldots,n$ and $p < n$.\n",
    "\n",
    "1. For a given regressor, let $\\hat y_i$ be prediction given $x_i$, and $\\hat y$ be the vector form.  Show that linear regression can be written in the form\n",
    "$$\n",
    "\\hat y = H y,\n",
    "$$\n",
    "where $H$ is dependent on $X$ (the matrix of where each row is $x_i$), assuming that $p < n$ and $X$ is full rank.  Give an expression for $H$ or an algorithm for computing $H$. \n",
    "\n",
    "2. Assuming $p < n$ and $X$ is full rank, let $X = U D V^\\top$ be the thin singular value decomposition where $U$ is $n \\times p$, and $V, D$ is $p \\times p$ ($D$ is diagonal). \n",
    "    - a) Derive an expression for the OLS coefficients $\\hat\\beta = A b$ such that $A$ is $p \\times p$ and depends on $V$ and $D$, and $b$ is a $p$ vector and does not depend on $D$.   \n",
    "    - b) Describe a fit method that precomputes these quantities separately\n",
    "    - c) Use the simulated data $y$ and $X$ in below to find $\\hat \\beta$ using SVD.\n",
    "    - d) Call a new data $\\tilde X \\in \\mathbb{R}^{m \\times p}$, derive an expression for the predicted $y$ with $\\tilde X$ using SVD. \n",
    "3. Consider a regressor that performs OLS using the SVD above, but every instance of D will only use the largest $r$ values on the diagonal, the remainder will be set to 0.  Call this new $p \\times p$ matrix $D_r$ ($r < p$).  Then the new coefficient vector is the OLS computed as if the design matrix is modified by $X \\gets U D_r V^\\top$.  \n",
    "    - a) Given that you have computed $b$ already, how could you make a method `change_rank` that recomputes $A$ with $D_r$ instead of $D$? \n",
    "    - b) Choose $r = 10$, recompute $\\hat\\beta$ (call it $\\hat\\beta_{\\text{LowRank}}$) in Question 2-c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced8a4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simulate from the linear regression model (use y,X)\n",
    "np.random.seed(2022)\n",
    "n, p = 100,20\n",
    "X = np.random.normal(0,1,size = (n,p))\n",
    "beta = np.random.normal(0,.2,size = (p))\n",
    "sigma = 1\n",
    "y = np.random.normal(X @ beta, sigma**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882a747b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.matrix_rank(X) # X is full rank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634fe776",
   "metadata": {},
   "source": [
    "### Exercise 3 (Subset selection)  (20 pts, 5 each)\n",
    "\n",
    "Recall the subset selection problem with tuning parameter $k$,\n",
    "$$\n",
    "\\min_{\\beta : \\| \\beta \\|_0 \\le k}\\| y - X \\beta \\|_2^2,\n",
    "$$\n",
    "where $\\|\\beta\\|_0 = \\#\\{j = 1\\,\\ldots,p : \\beta_j \\ne 0 \\}$.\n",
    "\n",
    "Notice that we can write this as \n",
    "$$\n",
    "\\min_{\\beta : |{\\rm supp}(\\beta)| \\le k}\\| y - X \\beta \\|_2^2,\n",
    "$$\n",
    "where \n",
    "${\\rm supp}(\\beta) = \\{j = 1\\,\\ldots,p : \\beta_j \\ne 0 \\}$ (${\\rm supp}(\\beta)$ is the support of $\\beta$).\n",
    "\n",
    "1. Write the subset selection problem in the following form\n",
    "$$\n",
    "\\min_{S \\subseteq \\{1,\\ldots,p\\}, |S|\\le k} y^\\top P_S y,\n",
    "$$\n",
    "where $P_S$ is a projection.  \n",
    "2. Suppose that we have a nested sequence of models $S_1\\subset S_2 \\subset \\ldots \\subset S_p$ such that $|S_k| = k$ ($|S_k|$ is the cardinality of $S_k$, meaning that it contains $k$ variables).  Prove that $$y^\\top P_{S_k} y \\ge y^\\top P_{S_{k+1}} y$$ for $k=1,\\ldots,p-1$.  What does this tell us about the solution to the subset selection problem and the constraint $|S| \\le k$?\n",
    "\n",
    "    (Hint: using the fact that $X^TX$ is positive definite, write $X^TX= VD^2V^T$)\n",
    "\n",
    "3. Suppose that $X$ is orthogonal, then write a computationally efficient pseudocode to solve the subset selection problem.  Prove that it is correct (your algorithm actually solves subset selection under othogonal design).\n",
    "4. (Challenge) Suppose that we have that $n = p$ and $y_i = \\beta_i + \\epsilon_i$ (identity design matrix) where $\\epsilon_i$ satisfies \n",
    "$$\n",
    "\\mathbb P \\left\\{ |\\epsilon_i| \\ge t \\right\\} \\le 2 e^{-t^2 / 2\\sigma^2}\n",
    "$$\n",
    "for any $t > 0$ (this is true for central Normal RVs) for some $\\sigma > 0$.\n",
    "Suppose that there is some true $S_0 \\subset\\{1,\\ldots,p\\}$ such that $|S_0| = k < p$ and ${\\rm supp}(\\beta) = S_0$.\n",
    "Prove the following.\n",
    "\n",
    "__Proposition__\n",
    "Define $\\mu = \\min_{j \\in S_0} |\\beta_j|$ and call $\\mu / \\sigma$ the signal-to-noise ratio.  Then if \n",
    "$$\n",
    "\\frac{\\mu}{\\sigma} > 2 \\sqrt{2 \\log \\left( \\frac{2n}{\\delta}\\right)},\n",
    "$$\n",
    "then the true $S$ is selected by subset selection with probability at least $1 - \\delta$.\n",
    "\n",
    "Hint: rewrite the subset selection problem with $X = I$ and compare the objective at $S_0$ to any other $S$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e8672a",
   "metadata": {},
   "source": [
    "### Exercise 4 (Ridge, lasso and adaptive lasso) (35 pts, 5 pts each)\n",
    "\n",
    "For this exercise, it may be helpful to use the `sklearn.linear_model` module.  I have also included a plotting tool for making the lasso path in ESL.\n",
    "\n",
    "1. Load the training and test data using the script below.  Fit OLS on the training dataset and compute the test error.  Throughout you do not need to compute an intercept but you should normalize the $X$ (divide by the column norms).\n",
    "2. Ridge regression: Train and tune ridge regression using a validation set (choose LOOCV) and compute the test error (square error loss).\n",
    "3. Fit the lasso path with lars to the data and compute the test error for each returned lasso coefficient.\n",
    "4. Perform 3 without the lasso modification generating the lars path.  Compare and contrast the lars path to the lasso path, what is the key difference.  Comment on when the active sets differ and how, if they do at all.\n",
    "5. Fit the lasso path with coordinate descent to the data. Compare the lasso path using coordinate descent with the path using lars. Comment on what you found.\n",
    "6. Extract each active set from the lasso path and recompute the restricted OLS for each.  Compute and compare the test error for each model.\n",
    "7. Read this website [click here](https://towardsdatascience.com/an-adaptive-lasso-63afca54b80d). Fit the adaptive lasso method to the data. Compare the test error between the adaptive lasso and lasso for each returned coefficient. Comment on what you found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4717250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lars(coefs, lines=False, title=\"Lars Path\"):\n",
    "    \"\"\"\n",
    "    Plot the lasso path where coefs is a matrix - the columns are beta vectors\n",
    "    \"\"\"\n",
    "    xx = np.sum(np.abs(coefs.T), axis=1)\n",
    "    xx /= xx[-1]\n",
    "    plt.plot(xx, coefs.T)\n",
    "    ymin, ymax = plt.ylim()\n",
    "    if lines:\n",
    "        plt.vlines(xx, ymin, ymax, linestyle='dashed')\n",
    "    plt.xlabel('|coef| / max|coef|')\n",
    "    plt.ylabel('Coefficients')\n",
    "    plt.title(title)\n",
    "    plt.axis('tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85814dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('hw1.data','rb') as f:\n",
    "    y_tr,X_tr,y_te,X_te = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
