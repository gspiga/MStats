{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1 \n",
    "## by Gianni Spiga"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 (Empirical risk minimization) (20 pts, 5 pts each)\n",
    "\n",
    "Consider Poisson model with rate parameter $\\lambda$ which has PMF,\n",
    "$$\n",
    "p(y|\\lambda) = \\frac{\\lambda^y}{y!} e^{-\\lambda},\n",
    "$$\n",
    "where $y = 0,1,\\ldots$ is some count variable.\n",
    "In Poison regression, we model $\\lambda = e^{\\beta^\\top x}$ to obtain $p(y | x,\\beta)$.\n",
    "\n",
    "1. Let the loss function for Poisson regression be $\\ell_i(\\beta) = - \\log p(y_i | x_i, \\beta)$ for a dataset consisting of predictor variables and count values $\\{x_i,y_i\\}_{i=1}^n$.  Here $\\propto$ means that we disregard any additive terms that are not dependent on $\\beta$.  Write an expression for $\\ell_i$ and derive its gradient. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{aligned}\n",
    "\\ell_i(\\beta) &= - \\log p(y_i | x_i, \\beta) \\\\\n",
    "&= -\\log (\\frac{(e^{\\beta^TX_i})^{y_i}}{y_i!} e^{-e^{\\beta^TX_i}})\\\\\n",
    "&= -(\\beta^Tx_iy_i - \\log(y_i!) -e^{\\beta^TX_i})\\\\\n",
    "&= -\\beta^Tx_iy_i + \\log(y_i!) + e^{\\beta^TX_i}\\\\\n",
    "&\\propto -\\beta^Tx_iy_i + e^{\\beta^TX_i}\\\\\n",
    "\\text{With Gradient}\\\\\n",
    "\\nabla \\ell_i(\\beta) &= - x_iy_i +  x_ie^{\\beta^TX_i}\\\\\n",
    "\\end{aligned}\n",
    "$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Show that the empirical risk $R_n(\\beta)$ is a convex function of $\\beta$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\nabla \\ell_i(\\beta) = - x_iy_i +  x_ie^{\\beta^TX_i} \\\\\n",
    "$$\n",
    "R_n(\\beta) = \\frac{1}{n}\\sum_{i = 1}^n \\ell_i(\\theta) = \\frac{1}{n}\\sum_{i = 1}^n - \\log p(y_i | x_i, \\beta) \\\\\n",
    "\\nabla^2 R_n(\\beta) = \\frac{1}{n}\\sum_{i = 1}^n  \\nabla^2 (- \\log p(y_i | x_i, \\beta))\\\\\n",
    "\\nabla R_n(\\beta) = \\frac{1}{n}\\sum_{i = 1}^n - x_iy_i +  x_ie^{\\beta^TX_i} \\\\\n",
    "\\nabla^2 R_n(\\beta) = \\frac{1}{n}\\sum_{i = 1}^n x_i x_i^Te^{\\beta^TX_i} = H  \\\\\n",
    "$$\n",
    "\n",
    "We can then define the Hessian as the following. \n",
    "\n",
    "$$\n",
    "H(\\beta) = \\begin{bmatrix}\n",
    "    \\frac{\\partial^{2} R}{\\partial \\beta_{1} \\partial \\beta_{1} } & \\ldots & \\frac{\\partial^{2} R}{\\partial \\beta_{i} \\partial \\beta_{p} } \\\\\n",
    "    \\vdots & \\ddots & \\vdots \\\\\n",
    "     \\frac{\\partial^{2} R}{\\partial \\beta_{p} \\partial \\beta_{1} } & \\ldots & \\frac{\\partial^{2} R}{\\partial \\beta_{p} \\partial \\beta_{p} } \\\\\n",
    "  \\end{bmatrix} \\\\\n",
    "  \\text{Which we now can define as ...} \\\\\n",
    "  H(\\beta) = x_i x_i^Te^{\\beta^TX_i}\n",
    "$$\n",
    "\n",
    "We must show the Hessian matrix is positive definite, we show that for any vector $\\mu > 0$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mu^TH\\mu \\geq 0  &= \\mu^T x_i x_i^Te^{\\beta^TX_i} \\mu \\\\\n",
    " &= e^{\\beta^TX_i} \\mu^T x_i x_i^T \\mu \\\\\n",
    " \\text{Let} \\ a &= \\mu^T x_i, b = x_i x_i^T,\\{a,b\\} \\in \\mathbb{R} > 0\\\\\n",
    "  &= e^{\\beta^TX_i} ab  > 0\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Thus the Hessian is positive definite which leads us to conclude that $R_n(\\beta)$ is strictly convex. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Consider the mapping $F_\\eta(\\beta) = \\beta - \\eta \\nabla R_n(\\beta)$ which is the iteration of gradient descent ($\\eta>0$ is called the learning parameter).  Show that at the minimizer of $R_n$, $\\hat \\beta$, we have that $F(\\hat \\beta) = \\hat \\beta$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "F_\\eta(\\hat{\\beta}) &= \\beta - \\eta \\nabla R_n(\\hat{\\beta}) \\\\\n",
    "&= \\beta - 0 \\\\\n",
    "&= \\beta \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. I have a script to simulate from this model below.  Implement the gradient descent algorithm above and show that with enough data ($n$ large enough) the estimated $\\hat \\beta$ approaches the true $\\beta$ (you can look at the sum of square error between these two vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.14159642  0.32078409 -0.25810144 -0.15741574 -0.00722751 -0.34875925\n",
      " -0.33806496 -0.28184467 -0.01672773  0.19386419  0.26891229 -0.00396877\n",
      "  0.08205065 -0.01824143 -0.12716232  0.01491068  0.14876994 -0.0989665\n",
      "  0.05624015  0.22847606]\n",
      "2.815558131604145\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.18286105],\n",
       "       [ 0.29255061],\n",
       "       [-0.23580272],\n",
       "       [-0.21647555],\n",
       "       [-0.0246921 ],\n",
       "       [-0.37453167],\n",
       "       [-0.3863345 ],\n",
       "       [-0.28376493],\n",
       "       [-0.01149554],\n",
       "       [ 0.15474706],\n",
       "       [ 0.26400161],\n",
       "       [-0.02215312],\n",
       "       [ 0.05063015],\n",
       "       [-0.03090263],\n",
       "       [-0.11134687],\n",
       "       [-0.01535282],\n",
       "       [ 0.13969634],\n",
       "       [-0.14053155],\n",
       "       [ 0.03151566],\n",
       "       [ 0.19255056]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "## Simulate from the Poisson regression model (use y,X)\n",
    "np.random.seed(2022)\n",
    "n, p = 1000,20\n",
    "X = np.random.normal(0,1,size = (n,p))\n",
    "beta = np.random.normal(0,.2,size = (p))\n",
    "lamb = np.exp(X @ beta)\n",
    "y = np.random.poisson(lamb)\n",
    "\n",
    "beta0 = np.random.normal(0.25, .5, size = (p))\n",
    "\n",
    "def gradDesc(x, y, eta, beta, maxiter = 10000, error = 10e-5):\n",
    "    i = 1\n",
    "    n = X.shape[0]\n",
    "    # Turn into column vectors\n",
    "    betaNew = beta0.reshape(-1,1)\n",
    "    betaOld = 0\n",
    "    y = y.reshape(-1,1)\n",
    "    print(np.linalg.norm(betaNew - betaOld))\n",
    "    while i <= maxiter and np.linalg.norm(betaNew - betaOld) > error:\n",
    "        #print(\"My i is \", i)\n",
    "        betaOld = betaNew\n",
    "        # Resent gradient for each iteration\n",
    "        Rn = 0\n",
    "\n",
    "        Rn = (1/n) * (X.T @ np.exp(X @ betaOld) - X.T @ y)\n",
    "        #print(Rn)\n",
    "        #for row in range(n): # summation over n\n",
    "        #    Rn = Rn + -1*X[row,:] * y[row] + X[row,:] * np.exp(betaOld.T @ X[row,:])\n",
    "        betaNew = betaOld - eta * Rn #gradientRn(Bold)\n",
    "        #print(\"Hey this is my beta new \" ,betaNew)\n",
    "        i += 1\n",
    "\n",
    "    return betaNew\n",
    "\n",
    "print(beta)\n",
    "\n",
    "gradDesc(X, y, 0.01,beta0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the regression setting in which $x_i \\in \\mathbb R^p$ and $y_i \\in \\mathbb R$, for $i=1,\\ldots,n$ and $p < n$.\n",
    "\n",
    "1. For a given regressor, let $\\hat y_i$ be prediction given $x_i$, and $\\hat y$ be the vector form.  Show that linear regression can be written in the form\n",
    "$$\n",
    "\\hat y = H y,\n",
    "$$\n",
    "where $H$ is dependent on $X$ (the matrix of where each row is $x_i$), assuming that $p < n$ and $X$ is full rank.  Give an expression for $H$ or an algorithm for computing $H$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In linear regression, we have:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{y} &= X\\hat{\\beta} \\\\\n",
    "\\hat{y} &= X (X^TX)^{-1}X^Ty \\\\\n",
    "\\hat{y} &= Hy \\\\\n",
    "\\text{where } H &= X (X^TX)^{-1}X^T\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Assuming $p < n$ and $X$ is full rank, let $X = U D V^\\top$ be the thin singular value decomposition where $U$ is $n \\times p$, and $V, D$ is $p \\times p$ ($D$ is diagonal). \n",
    "    - a) Derive an expression for the OLS coefficients $\\hat\\beta = A b$ such that $A$ is $p \\times p$ and depends on $V$ and $D$, and $b$ is a $p$ vector and does not depend on $D$.   \n",
    "    - b) Describe a fit method that precomputes these quantities separately\n",
    "    - c) Use the simulated data $y$ and $X$ in below to find $\\hat \\beta$ using SVD.\n",
    "    - d) Call a new data $\\tilde X \\in \\mathbb{R}^{m \\times p}$, derive an expression for the predicted $y$ with $\\tilde X$ using SVD. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a.)\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{\\beta} &= ((U D V^\\top)^TU D V^\\top)^{-1}(U D V^\\top)^Ty\\\\\n",
    "            &= (V D U^\\top U D V^\\top)^{-1}(U D V^\\top)^Ty\\\\\n",
    "            &= (V D U^\\top U D V^\\top)^{-1}(V D U^\\top)^Ty\\\\\n",
    "            &= (V D D V^\\top)^{-1}V D U^\\top y\\\\\n",
    "            &= (V^\\top)^{-1} D^{-1} D^{-1} V^{-1} V D U^\\top y\\\\\n",
    "            &= V D^{-1} U^\\top y\\\\\n",
    "            &= Ab \\\\\n",
    "            \\text{where } A &= V D^{-1}, b= U^\\top y, A, \\ b \\in \\mathbb{R}^p\n",
    "\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b.) \n",
    "\n",
    "One could use numpy's linalg.svd to solve for the matrices $U$,$D$, and $V$ to which they could be then computed to solve for $A$ and $b$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.42325615 -0.15765029  0.1264379  -0.22332322  0.40640595 -0.1126941\n",
      " -0.17207485 -0.29528352  0.05380933  0.11729673  0.08383002 -0.44454209\n",
      "  0.12996738 -0.42292366  0.25061563 -0.58863292 -0.29211494 -0.03368431\n",
      " -0.37213045  0.02890809]\n"
     ]
    }
   ],
   "source": [
    "U, D, V = np.linalg.svd(X, full_matrices=False)\n",
    "A = V @ np.diag(1/D)\n",
    "b = U.T @ y\n",
    "betaSVD = A @ b \n",
    "print(betaSVD)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d.)\n",
    "\n",
    "Let the SVD of $\\tilde{X}$ be $\\tilde{U}, \\tilde{D}, \\tilde{V}$, then we can define the predicted values of $\\hat{y}$ with the following:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{y} &= \\tilde{X} \\hat{\\beta} \\\\\n",
    "        &= \\tilde{U} \\tilde{D} \\tilde{V}^T \\hat{\\beta} \\\\\n",
    "        &= \\tilde{U} \\tilde{D} \\tilde{V}^T \\tilde{V} \\tilde{D}^{-1} \\tilde{U}^T y \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Consider a regressor that performs OLS using the SVD above, but every instance of D will only use the largest $r$ values on the diagonal, the remainder will be set to 0.  Call this new $p \\times p$ matrix $D_r$ ($r < p$).  Then the new coefficient vector is the OLS computed as if the design matrix is modified by $X \\gets U D_r V^\\top$.  \n",
    "    - a) Given that you have computed $b$ already, how could you make a method `change_rank` that recomputes $A$ with $D_r$ instead of $D$? \n",
    "    - b) Choose $r = 10$, recompute $\\hat\\beta$ (call it $\\hat\\beta_{\\text{LowRank}}$) in Question 2-c.\n",
    "\n",
    "### a.)\n",
    "\n",
    "In our matrix $D$, the entries of singular values are already ordered from largest to smallest, so if we wanted to use the $r$ largest singular values, we'd only have to take the first $r$ entries of the diagonal. This would be easily implementable into a function `change_rank`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simulate from the linear regression model (use y,X)\n",
    "np.random.seed(2022)\n",
    "n, p = 100,20\n",
    "X = np.random.normal(0,1,size = (n,p))\n",
    "beta = np.random.normal(0,.2,size = (p))\n",
    "sigma = 1\n",
    "y = np.random.normal(X @ beta, sigma**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_rank(X, y, r):\n",
    "    U,D,V  = np.linalg.svd(X, full_matrices = False)\n",
    "    p = U.shape[1]\n",
    "    np.put(D, np.arange(10,20), np.repeat(0, p - r))\n",
    "    A = V @ np.diag(D)\n",
    "    b = U.T @ y \n",
    "    beta = A @ b\n",
    "    return beta\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -1.26673703,  -0.3288066 ,  14.64499225,  -7.47486784,\n",
       "        23.71705398,  -0.87308367,  29.21796126,   3.17425597,\n",
       "         1.45353551,  -3.67764043,   6.38765624,   7.89133118,\n",
       "         5.18706178,   7.05430433, -28.91108728,   1.04644405,\n",
       "        -2.61613846,  -5.61902513,  -4.51245147,   7.38914609])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "change_rank(X, y, r = 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the subset selection problem with tuning parameter $k$,\n",
    "$$\n",
    "\\min_{\\beta : \\| \\beta \\|_0 \\le k}\\| y - X \\beta \\|_2^2,\n",
    "$$\n",
    "where $\\|\\beta\\|_0 = \\#\\{j = 1\\,\\ldots,p : \\beta_j \\ne 0 \\}$.\n",
    "\n",
    "Notice that we can write this as \n",
    "$$\n",
    "\\min_{\\beta : |{\\rm supp}(\\beta)| \\le k}\\| y - X \\beta \\|_2^2,\n",
    "$$\n",
    "where \n",
    "${\\rm supp}(\\beta) = \\{j = 1\\,\\ldots,p : \\beta_j \\ne 0 \\}$ (${\\rm supp}(\\beta)$ is the support of $\\beta$).\n",
    "\n",
    "1. Write the subset selection problem in the following form\n",
    "$$\n",
    "\\min_{S \\subseteq \\{1,\\ldots,p\\}, |S|\\le k} y^\\top P_S y,\n",
    "$$\n",
    "where $P_S$ is a projection.  \n",
    "\n",
    "\n",
    "**Answer**: Currently, we are looking to minimize the residual sum of squares, where we can expand the loss function as: \n",
    "$$\n",
    "\\| y - X \\beta \\|_2^2 = (y - X \\beta)^T(y - X \\beta)\n",
    "$$\n",
    "\n",
    "The classical solution to the least squares problem is the Moore-Penrose psuedo-inverse such that:\n",
    "$\n",
    "\\hat{\\beta} = (X^TX)^{-1}X^Ty\n",
    "$.\n",
    "However, we want a projector such that:\n",
    "$\n",
    "X\\beta = P_S y\n",
    "$\n",
    "leading us to now minimize:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\|y-\\mathbb P_S y\\|_2^2=\\|(I-\\mathbb P_S)y\\|_2^2 \\\\\n",
    "= y^T(I-\\mathbb P_S)y \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    " Where the mimizing projection is $\\mathbb P_S = X(X^TX)^{-1}X^T$. We seek to minimize the residual sum of squares by finding the support of $\\beta$, the subset which is not mapped to zero in the space $S$. We can use the projection to the orthogonal complement of the subspace. \n",
    "$$\n",
    "\\min_{S \\subseteq \\{1,\\ldots,p\\}, |S|\\le k} y^\\top (I-\\mathbb P_S) y \\\\\n",
    " = \\min_{S \\subseteq \\{1,\\ldots,p\\}, |S|\\le k} y^\\top (I - X(X^TX)^{-1}X^T) y \\\\\n",
    " = \\min_{S \\subseteq \\{1,\\ldots,p\\}, |S|\\le k} y^\\top (I - UDV^T(VD^2V^T)^{-1}(UDV^T)^T) y \\\\\n",
    "  = \\min_{S \\subseteq \\{1,\\ldots,p\\}, |S|\\le k} y^\\top (I - (UD^{-1}V^T)(UDV^T)^T) y \\\\\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Suppose that we have a nested sequence of models $S_1\\subset S_2 \\subset \\ldots \\subset S_p$ such that $|S_k| = k$ ($|S_k|$ is the cardinality of $S_k$, meaning that it contains $k$ variables).  Prove that $$y^\\top P_{S_k} y \\ge y^\\top P_{S_{k+1}} y$$ for $k=1,\\ldots,p-1$.  What does this tell us about the solution to the subset selection problem and the constraint $|S| \\le k$?\n",
    "\n",
    "    (Hint: using the fact that $X^TX$ is positive definite, write $X^TX= VD^2V^T$)\n",
    "\n",
    "**Answer** \n",
    "$$\n",
    "\\begin{aligned}\n",
    "y^\\top P_{S_k} y &\\ge y^\\top P_{S_{k+1}} y \\\\\n",
    "y^\\top (I - X(VD^2V^T)^{-1}X^T) y &\\ge y^\\top (I - \\tilde{X}(\\tilde{X}^T\\tilde{X})^{-1}\\tilde{X}^T) y\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Suppose that $X$ is orthogonal, then write a computationally efficient pseudocode to solve the subset selection problem.  Prove that it is correct (your algorithm actually solves subset selection under othogonal design)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. (Challenge) Suppose that we have that $n = p$ and $y_i = \\beta_i + \\epsilon_i$ (identity design matrix) where $\\epsilon_i$ satisfies \n",
    "$$\n",
    "\\mathbb P \\left\\{ |\\epsilon_i| \\ge t \\right\\} \\le 2 e^{-t^2 / 2\\sigma^2}\n",
    "$$\n",
    "for any $t > 0$ (this is true for central Normal RVs) for some $\\sigma > 0$.\n",
    "Suppose that there is some true $S_0 \\subset\\{1,\\ldots,p\\}$ such that $|S_0| = k < p$ and ${\\rm supp}(\\beta) = S_0$.\n",
    "Prove the following.\n",
    "\n",
    "__Proposition__\n",
    "Define $\\mu = \\min_{j \\in S_0} |\\beta_j|$ and call $\\mu / \\sigma$ the signal-to-noise ratio.  Then if \n",
    "$$\n",
    "\\frac{\\mu}{\\sigma} > 2 \\sqrt{2 \\log \\left( \\frac{2n}{\\delta}\\right)},\n",
    "$$\n",
    "then the true $S$ is selected by subset selection with probability at least $1 - \\delta$.\n",
    "\n",
    "Hint: rewrite the subset selection problem with $X = I$ and compare the objective at $S_0$ to any other $S$.\n",
    "\n",
    "**Answer**\n",
    "\n",
    "$$\n",
    "\\beta_{S_0} = \n",
    "\\begin{cases}\n",
    "  \\beta_i & \\text{if } i \\in S_0 \\\\\n",
    "  0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$\\forall$ support set in $S$:\n",
    "$$\n",
    "P(\\| Y - \\beta_{S_0} \\|_2^2 \\leq \\| Y - \\beta_{S} \\|_2^2 ) \\geq 1 - \\delta \\\\\n",
    "P(\\| Y - \\beta_{S_0} \\|_2^2 - \\| Y - \\beta_{S} \\|_2^2 \\leq 0) \\geq 1 - \\delta \\\\\n",
    "P(\\| \\beta_{S_0} + \\epsilon - \\beta_S \\|_2^2 - \\| \\epsilon \\|_2^2 \\leq 0) \\geq 1 - \\delta \\\\\n",
    "P(\\| \\beta_{S_0} - \\beta_S \\|_2^2 \\| + 2(\\beta_{S_0} - \\beta_S)^T\\epsilon + \\| \\epsilon \\|_2^2 -  \\|\\epsilon \\|_2^2 \\geq 0) \\geq 1 - \\delta \\\\\n",
    "P(\\| \\beta_{S_0} - \\beta_S \\|_2^2 \\| + 2(\\beta_{S_0} - \\beta_S)^T\\epsilon \\geq 0) \\geq 1 - \\delta \\\\\n",
    "\\mu^2 \\leq P(\\| \\beta_{S_0} - \\beta_S \\|_2^2 \\| + 2(\\beta_{S_0} - \\beta_S)^T\\epsilon \\leq 0) \\leq \\delta \\\\\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4\n",
    "\n",
    "For this exercise, it may be helpful to use the `sklearn.linear_model` module.  I have also included a plotting tool for making the lasso path in ESL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the training and test data using the script below.  Fit OLS on the training dataset and compute the test error.  Throughout you do not need to compute an intercept but you should normalize the $X$ (divide by the column norms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('hw1.data','rb') as f:\n",
    "    y_tr,X_tr,y_te,X_te = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0649470897490387"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the OLS model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "X_tr = Normalizer().transform(X_tr)\n",
    "X_te = Normalizer().transform(X_te)\n",
    "\n",
    "OLS = LinearRegression().fit(X_tr, y_tr)\n",
    "OLSpred = OLS.predict(X_te)\n",
    "mean_squared_error(y_te, OLSpred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Ridge regression: Train and tune ridge regression using a validation set (choose LOOCV) and compute the test error (square error loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
