{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1 \n",
    "## by Gianni Spiga"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 (Empirical risk minimization) (20 pts, 5 pts each)\n",
    "\n",
    "Consider Poisson model with rate parameter $\\lambda$ which has PMF,\n",
    "$$\n",
    "p(y|\\lambda) = \\frac{\\lambda^y}{y!} e^{-\\lambda},\n",
    "$$\n",
    "where $y = 0,1,\\ldots$ is some count variable.\n",
    "In Poison regression, we model $\\lambda = e^{\\beta^\\top x}$ to obtain $p(y | x,\\beta)$.\n",
    "\n",
    "1. Let the loss function for Poisson regression be $\\ell_i(\\beta) = - \\log p(y_i | x_i, \\beta)$ for a dataset consisting of predictor variables and count values $\\{x_i,y_i\\}_{i=1}^n$.  Here $\\propto$ means that we disregard any additive terms that are not dependent on $\\beta$.  Write an expression for $\\ell_i$ and derive its gradient. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{aligned}\n",
    "\\ell_i(\\beta) &= - \\log p(y_i | x_i, \\beta) \\\\\n",
    "&= -\\log (\\frac{(e^{\\beta^TX_i})^{y_i}}{y_i!} e^{-e^{\\beta^TX_i}})\\\\\n",
    "&= -(\\beta^Tx_iy_i - \\log(y_i!) -e^{\\beta^TX_i})\\\\\n",
    "&= -\\beta^Tx_iy_i + \\log(y_i!) + e^{\\beta^TX_i}\\\\\n",
    "&\\propto -\\beta^Tx_iy_i + e^{\\beta^TX_i}\\\\\n",
    "\\text{With Gradient}\\\\\n",
    "\\nabla \\ell_i(\\beta) &= - x_iy_i +  x_ie^{\\beta^TX_i}\\\\\n",
    "\\end{aligned}\n",
    "$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Show that the empirical risk $R_n(\\beta)$ is a convex function of $\\beta$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\nabla \\ell_i(\\beta) = - x_iy_i +  x_ie^{\\beta^TX_i} \\\\\n",
    "$$\n",
    "R_n(\\beta) = \\frac{1}{n}\\sum_{i = 1}^n \\ell_i(\\theta) = \\frac{1}{n}\\sum_{i = 1}^n - \\log p(y_i | x_i, \\beta) \\\\\n",
    "\\nabla^2 R_n(\\beta) = \\frac{1}{n}\\sum_{i = 1}^n  \\nabla^2 (- \\log p(y_i | x_i, \\beta))\\\\\n",
    "\\nabla R_n(\\beta) = \\frac{1}{n}\\sum_{i = 1}^n - x_iy_i +  x_ie^{\\beta^TX_i} \\\\\n",
    "\\nabla^2 R_n(\\beta) = \\frac{1}{n}\\sum_{i = 1}^n x_i x_i^Te^{\\beta^TX_i} = H  \\\\\n",
    "$$\n",
    "\n",
    "We can then define the Hessian as the following. \n",
    "\n",
    "$$\n",
    "H(\\beta) = \\begin{bmatrix}\n",
    "    \\frac{\\partial^{2} R}{\\partial \\beta_{1} \\partial \\beta_{1} } & \\ldots & \\frac{\\partial^{2} R}{\\partial \\beta_{i} \\partial \\beta_{p} } \\\\\n",
    "    \\vdots & \\ddots & \\vdots \\\\\n",
    "     \\frac{\\partial^{2} R}{\\partial \\beta_{p} \\partial \\beta_{1} } & \\ldots & \\frac{\\partial^{2} R}{\\partial \\beta_{p} \\partial \\beta_{p} } \\\\\n",
    "  \\end{bmatrix} \\\\\n",
    "  \\text{Which we now can define as ...} \\\\\n",
    "  H(\\beta) = x_i x_i^Te^{\\beta^TX_i}\n",
    "$$\n",
    "\n",
    "We must show the Hessian matrix is positive definite, we show that for any vector $\\mu > 0$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mu^TH\\mu \\geq 0  &= \\mu^T x_i x_i^Te^{\\beta^TX_i} \\mu \\\\\n",
    " &= e^{\\beta^TX_i} \\mu^T x_i x_i^T \\mu \\\\\n",
    " \\text{Let} \\ a &= \\mu^T x_i, b = x_i x_i^T,\\{a,b\\} \\in \\mathbb{R} > 0\\\\\n",
    "  &= e^{\\beta^TX_i} ab  > 0\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Thus the Hessian is positive definite which leads us to conclude that $R_n(\\beta)$ is strictly convex. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Consider the mapping $F_\\eta(\\beta) = \\beta - \\eta \\nabla R_n(\\beta)$ which is the iteration of gradient descent ($\\eta>0$ is called the learning parameter).  Show that at the minimizer of $R_n$, $\\hat \\beta$, we have that $F(\\hat \\beta) = \\hat \\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. I have a script to simulate from this model below.  Implement the gradient descent algorithm above and show that with enough data ($n$ large enough) the estimated $\\hat \\beta$ approaches the true $\\beta$ (you can look at the sum of square error between these two vectors)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
