{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4\n",
    "\n",
    "## Gianni Spiga\n",
    "\n",
    "### For STA 209, Fall 2023\n",
    "\n",
    "- I discussed approaches and methodology to these problems with Jonas Kempf and Niraj Bangari as well as the assistance of the professor and Mathew Chen during Office Hours. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We consider the quadratic objective function on $\\mathbb{R}^2$\n",
    "$$\n",
    "f (x) = \\frac{1}{2}(x^2_1 + \\gamma x_2^2),\n",
    "$$\n",
    "where $\\gamma > 0$. Clearly, the optimal point is $x = 0$, and the optimal value is $0$. We apply the gradient descent method with $\\alpha_k = \\alpha = \\frac{2}{1 + \\gamma}$ at the point $x(0) = (\\gamma, 1)$.\n",
    "\n",
    "- (a) Derive the closed-form expressions for the $k$-th iterates $x(k)$ and function value $f (x(k))$.\n",
    "\n",
    "We first find our gradient for the updating step:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla f(x) &= x_1 + \\gamma x_2 \\\\ \n",
    "&= \\begin{bmatrix} 1 & 0 \\\\\n",
    "0 & \\gamma \\\\\n",
    "\\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Now we can plug in this into the form of our gradient descent updating step:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "x^{(k+1)} &= x^{(k)} - \\frac{2}{1 + \\gamma} \\nabla f(x) \\\\\n",
    "&= x^{(k)} - \\frac{2}{1 + \\gamma} \\begin{bmatrix} 1 & 0 \\\\ 0 & \\gamma \\\\\n",
    "\\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\end{bmatrix} \\\\\n",
    "&= \\big(I - \\frac{2}{1 + \\gamma} \\begin{bmatrix} 1 & 0 \\\\ 0 & \\gamma \\\\\n",
    "\\end{bmatrix}\\big) x^{(k)} \\\\\n",
    "&= \\big(I - \\frac{2}{1 + \\gamma} Q\\big) x^{(k)} \\\\\n",
    "&= \\big(I - \\begin{bmatrix} \\frac{2}{1 + \\gamma} & 0 \\\\ 0 & \\frac{2}{1 + \\gamma}\\gamma \\\\\n",
    "\\end{bmatrix}\\big) x^{(k)} \\\\\n",
    "&= \\big(\\begin{bmatrix} 1 - \\frac{2}{1 + \\gamma} & 0 \\\\ 0 & 1 - \\frac{2 \\gamma}{1 + \\gamma} \\\\\n",
    "\\end{bmatrix}\\big) x^{(k)} \\\\\n",
    "&= \\big(\\begin{bmatrix} 1 - \\frac{2}{1 + \\gamma} & 0 \\\\ 0 & 1 - \\frac{2 \\gamma}{1 + \\gamma} \\\\\n",
    "\\end{bmatrix} x^{(k)} \\\\\n",
    "&= \\big(\\begin{bmatrix} \\frac{\\gamma - 1}{1 + \\gamma} & 0 \\\\ 0 &  \\frac{1 - \\gamma}{1 + \\gamma} \\\\\n",
    "\\end{bmatrix} x^{(k)} \\\\\n",
    "&= A x^{(k)}\n",
    "\\text{Each update is a nested product series of $A$ times the previous $x^{(k)}$} \\\\\n",
    "x^{(k)} &= Ax^{(k - 1)} = A \\big(Ax^{(k - 2)}\\big) = \\ldots = A^k x^{(0)} \\\\\n",
    "&= \\begin{bmatrix} \\big(\\frac{\\gamma - 1}{1 + \\gamma}\\big)^k \\gamma\\\\ \\big(\\frac{1 - \\gamma}{1 + \\gamma}\\big)^k  \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- (b) Discuss the convergence rate $f (x^{(T )}) − f (x∗)$ with different values of $\\gamma$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "f(x^T) &= \\frac{1}{2}\\bigg[\\gamma^2 \\big(\\frac{\\gamma - 1}{1 + \\gamma}\\big)^{2T} + \\gamma \\big(\\frac{1 - \\gamma}{1 + \\gamma}\\big)^{2T} \\bigg] \\\\\n",
    "&= \\frac{\\gamma}{2(1 + \\gamma)^{2T}} \\bigg[\\gamma \\big(\\gamma - 1\\big)^{2T} + \\big(1 - \\gamma\\big)^{2T} \\bigg] \\\\\n",
    "&= \\frac{\\gamma ( 1 + \\gamma)}{2(1 + \\gamma)^{2T}} \\bigg[\\big(\\gamma - 1\\big)^{2T} \\bigg] \\\\\n",
    "&= \\frac{\\gamma ( 1 + \\gamma)}{2} \\frac{\\big(\\gamma - 1\\big)^{2T}}{(1 + \\gamma)^{2T}} \\\\\n",
    "&\\text{We have} \\\\\n",
    "&= \\frac{\\big(\\gamma - 1\\big)^{2T}}{(1 + \\gamma)^{2T}} f(x^{(0)})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "As made clear by the fraction term, when $\\gamma$ is close to $1$, our step size becomes small, making the convergence very slow. However, when our $\\gamma$ is closer to $0$, we have much faster convergence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. We would like to define the direction of steepest descent with respect to some norm $‖ · ‖$ at a point $x$. A first attempt to define this might be\n",
    "$$\n",
    "\\begin{equation}\n",
    "∆x = \\lim_{t→0} \\argmin_{‖v‖=t} f (x + v) \n",
    "\\end{equation}\n",
    "$$\n",
    "However this does not work because the limit is not well-defined if the argmin is not unique. Under the assumptions that the argmin is unique and that f is smooth, we can rearrange Equation (1) to find\n",
    "$$\n",
    "\\begin{align*}\n",
    "∆x &= \\lim_{t→0} \\arg \\min_{‖v‖=t} f (x + v) = \\arg \\min_{‖v‖=1} \\lim_{t→0} \\frac{f (x + tv) − f (x)}{t} \\\\\n",
    "&= \\arg \\min_{‖v‖=1} ∇f (x)^T v .\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This last expression is well-defined even if the argmin is not unique or if $f$ is not smooth.\n",
    "\n",
    "**Definition**: We defined the **directions of steepest descent** on $f$ with respect to a\n",
    "norm $‖ · ‖$ at $x$ as:\n",
    "$$\n",
    "∆x = \\argmin_{‖v‖=1}∇f (x)^T v .\n",
    "$$\n",
    "There may be many directions of steepest descent if there are many $v$ achieving the mini-\n",
    "mum.\n",
    "\n",
    "For each of the following norms, identify the direction(s) of steepest descent, providing a\n",
    "simple charactarization of the direction(s) $∆x$ in terms of the gradient $∇f (x)$.\n",
    "\n",
    " - (a) For $H ∈ \\mathbb{R}^{n×n}, H \\succ 0$, consider the norm $‖x‖^2_H = x^T Hx$. What are the directions of\n",
    "steepest descent on $f$ with respect to this norm at a point $x$?\n",
    "\n",
    "We seek the $\\argmin_{‖v‖_H=1}\\nabla f(x)^T v$. We consider a change of variable, so that our constraint can be $v^THv = 1$, such as:\n",
    "\n",
    "$$\n",
    "u = H^{1/2} v \\\\\n",
    "\\text{We minimize:} \\\\\n",
    "\\nabla f(x)^T  H^{1/2} u \\\\\n",
    "$$\n",
    "\n",
    "The minimizing vector is that in the opposite direction thus $-f(x)^T H^{1/2}$, however to meet our constraint, our minimizer is:\n",
    "$$\n",
    "u* =  \\frac{-f(x)^T H^{1/2}}{\\|f(x)^T H^{1/2}\\|_2} \\\\\n",
    "$$\n",
    "\n",
    "which is the direction of steepest descent for our norm. \n",
    "\n",
    "\n",
    "- (b) What are the direction(s) of steepest descent on $f$ w.r.t. the $l_1$ norm at a point $x$?\n",
    "\n",
    "We look to maximize the $i^*$ direction of the steepest decent. \n",
    "$$\n",
    "i^* =max_i |\\nabla f_i (x)| \\\\\n",
    "\\text{Which would be in the case of the $l_1$ norm...} \\\\\n",
    "\\delta^* = -(sign \\nabla_* f(x))e_{i*}\n",
    "$$\n",
    "\n",
    "where $e_{i*}$ is the unit vector in the $i^*$th direction. The steepest direction might not be unique as the gradient might maximize in multiple directions.\n",
    "\n",
    "\n",
    "- (c) What are the direction(s) of steepest descent on f w.r.t. the $l_{\\infty}$ norm at a point $x$?\n",
    "\n",
    "We look to maximize the $i^*$ direction of the steepest decent. \n",
    "$$\n",
    "i^* =max_i |\\nabla f_i (x)| \\\\\n",
    "\\text{Which would be in the case of the $l_{\\infty}$ norm...} \\\\\n",
    "\\delta^* = -(sign \\nabla_* f(x))e_{i*}\n",
    "$$\n",
    "\n",
    "While this answer is the same as the $l_1$ norm, there is a slight difference. The $l_1$-unit ball is a diamond, meaning other directions can lie on the boundary of the ball. However, since the the $l_{\\infty}$ is a cube, there can be no other directions that can lie on the boundary of the steepest descent direction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. We will define strong convexity more generally than in Boyd and Vandenberghe (Section w9.1.2). In particular, our definition is also valid for non-differentiable functions.\n",
    "\n",
    "Definition: A function $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is $\\mu$-strongly convex if for every $x, y ∈ \\mathbb{R}^n$ and every\n",
    "$θ ∈ [0, 1]$:\n",
    "$$\n",
    "f((1 − θ)x + θy) \\leq (1 − θ)f (x) + θf (y) − \\frac{\\mu}{2} θ(1 − θ)‖x − y‖^2_2.\n",
    "$$\n",
    "Remark: The definition is applicable also to functions $f : \\mathbb{R}^n \\rightarrow \\mathbb{R} \\cup \\{\\infty\\}$, in which case\n",
    "the inequality implies the domain of the function is convex. In this problem, we will only refer to functions with a domain of Rn.\n",
    "\n",
    "- (a) Prove that a continuously differentiable function $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is μ-strongly convex if\n",
    "and only if for every $x, y ∈ \\mathbb{R}^n$,\n",
    "$$\n",
    "f (y) ≥ f (x) + ∇f (x)^T (y − x) + \\frac{μ}{2}‖y − x‖^2_2. \\ \n",
    "$$\n",
    "This generalizes the first order characterization of convexity (Section 3.1.3).\n",
    "\n",
    "**Forward** We know we have strong convexity, so we can start with the following:\n",
    "$$\n",
    "\\begin{align*}\n",
    "f(y) &\\geq f(x_{\\theta}) +  \\nabla f(x_{\\theta})^T (y - x_{\\theta}) + \\frac{\\mu}{2}\\|y-x\\|^2_2 \\\\\n",
    "&= f(x_{\\theta}) + (1 - \\theta) \\nabla f(x_{\\theta})^T (y - x) + \\frac{\\mu}{2}(1 - \\theta)^2\\|y-x\\|^2_2 \\\\\n",
    "&\\text{and} \\\\\n",
    "f(x) &\\geq f(x_{\\theta}) +  \\nabla f(x_{\\theta})^T (x - x_{\\theta}) + \\frac{\\mu}{2}\\|y-x\\|^2_2 \\\\\n",
    "&= f(x_{\\theta}) - \\theta \\nabla f(x_{\\theta})^T (y - x) + \\frac{\\mu}{2}\\theta^2\\|y-x\\|^2_2 \\\\\n",
    "&\\text{We combine $\\theta$ times the first inequality with $(1 - \\theta)$ times the second inequality} \\\\\n",
    "\\theta f(y) + (1 - \\theta) f(x) &\\leq \\theta f(x_{\\theta}) + (1 - \\theta)f(x_{\\theta}) + [\\theta(1-\\theta) - \\theta(1-\\theta) ] \\nabla f(x_{\\theta})^T (y - x) + \\frac{\\mu}{2}[\\theta(1-\\theta)^2 + (1 - \\theta) \\theta^2] \\|y - x\\|^2_2 \\\\\n",
    "&\\leq f(x_{\\theta}) + \\frac{\\mu}{2}[\\theta(1-\\theta)^2 + (1 - \\theta) \\theta^2] \\|y - x\\|^2_2 \\\\\n",
    "&\\leq f(x_{\\theta}) + \\frac{\\mu}{2}[(1 - \\theta) \\theta] \\|y - x\\|^2_2 \\\\\n",
    "f(x_{\\theta}) &\\leq \\theta f(y) + (1 - \\theta) f(x) - \\frac{\\mu}{2}[(1 - \\theta) \\theta] \\|y - x\\|^2_2 \\\\\n",
    "f(\\theta y + (1-\\theta)x) &\\leq \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "**Backwards** If $f$ is $\\mu$-strongly convex, then we have the inequality:\n",
    "$$\n",
    "f (y) ≥ f (x) + ∇f (x)^T (y − x) + \\frac{μ}{2}‖y − x‖^2_2. \\ \n",
    "$$\n",
    "whcih by definition is trivially, strongly convex. \n",
    "\n",
    "- (b) Prove that a twice continuously differentiable function $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is $\\mu$-strongly\n",
    "convex if and only if for every $x \\in \\mathbb{R}^n$, ll eigenvalues of the Hessian at $x$ are greater\n",
    "or equal to $μ$, i.e.,\n",
    "$$\n",
    "\\nabla^2 f (x) \\succeq μI .\n",
    "$$\n",
    "\n",
    "**Forward** By Taylor's Theorem for $f(x+p)$ where $p = y - x$:\n",
    "$$\n",
    "\\begin{align*}\n",
    "f(x + p) &= f(x) + \\nabla f(x)^T p + \\frac{1}{2} (p)^T \\nabla^2 f(x + \\gamma p) p,  \\ \\gamma \\in (0,1) \\\\\n",
    "f(x) + \\nabla f(x)^T p + \\frac{\\mu}{2} \\|p\\|^2_2 &\\leq \\\\\n",
    "\\mu \\|p\\|^2_2 &\\leq  (p)^T \\nabla^2 f(x + \\gamma p) p \\\\\n",
    "&\\text{Let $p = \\alpha u$}\\\\ \n",
    "\\alpha^2 \\mu \\|u\\|^2_2 &\\leq  \\alpha^2 (u)^T \\nabla^2 f(x + \\gamma \\alpha u ) u \\\\\n",
    "&\\text{By continuity of $\\nabla^2 f$:} \\\\\n",
    "(u)^T \\nabla^2 f(x + \\gamma \\alpha u ) u &= \\lim_{\\alpha \\rightarrow 0} (u)^T \\nabla^2 f(x + \\gamma \\alpha u ) u \\geq  \\mu \\|u\\|^2_2 \\\\\n",
    "\\nabla^2 f(x) &\\succeq μI \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "**Backwards**\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "f(x + p) &= f(x) + \\nabla f(x)^T p + \\frac{1}{2} (p)^T \\nabla^2 f(x + \\gamma p) p,  \\ \\gamma \\in (0,1) \\\\\n",
    "&\\text{We know:} \\\\\n",
    "\\frac{1}{2} (p)^T \\nabla^2 f(x + \\gamma p) p &\\geq \\mu p^Tp \\\\\n",
    "&\\text{Thus the function is $\\mu$-strongly convex}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- (c) (c) Provide an example of a function $f : \\mathbb{R} \\rightarrow \\mathbb{R}$ that is strongly convex but not every-\n",
    "where differentiable.\n",
    "\n",
    "One example of a function that is strongly convex but not differentiable everywhere is:\n",
    "$$\n",
    "f(x) = x^2 + |x|\n",
    "$$\n",
    "\n",
    "This function has a positive second derivative, thus it is strongly convex. However, because of the absolute value term, it can not be differentiable at all values of $x$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Here we will assume nothing about convexity of $f$. We will show that gradient descent reaches an \\epsilon-substationary point x, such that $‖∇f (x)‖_2 ≤ \\epsilon$, in $O (1/\\epsilon^2)$ iterations. Important note: you may use here that\n",
    "$$\n",
    "f(y) ≤ f (x) + ∇f (x)^T (y − x) + \\frac{L}{2} \\|y − x\\|^2_2, \\ \\text{for all } x, y. \\ \\ \\ (3)\n",
    "$$\n",
    "Recall that you assumed convexity and twice differentiability of $f$ to show that the above is equivalent to the $L$-Lipschitz condition on $∇f$. But (3) is in fact a consequence of $∇f$ being $L$-Lipschitz, and does not actually require convexity or twice differentiability of $f$. We will write a generic update as $x^+ = x − t∇f (x)$, where $t ≤ 1/L$.\n",
    "\n",
    "- (a) Plug in $y = x^+ = x − t∇f (x)$ to (3) to show that\n",
    "$$\n",
    "f (x^+) ≤ f (x) − \\bigg(1 − \\frac{Lt}{2} \\bigg) t \\| ∇f (x) \\|^2_2\n",
    "$$\n",
    "We plug in:\n",
    "$$\n",
    "f(x − t∇f (x)) ≤ f (x) + ∇f (x)^T (x − t∇f (x) − x) + \\frac{L}{2} \\|x − t∇f (x) − x\\|^2_2, \\\\\n",
    "f(x^+) ≤ f (x) + ∇f (x)^T (− t∇f (x)) + \\frac{L}{2} \\|− t∇f (x)\\|^2_2, \\\\\n",
    " \\leq f (x) -t  \\|∇f (x)\\|^2_2 + \\frac{L}{2} t^2 \\|∇f (x)\\|^2_2, \\\\\n",
    " \\leq f (x) - \\big(t   + \\frac{L}{2} t^2\\big) \\|∇f (x)\\|^2_2, \\\\\n",
    " \\leq f (x) - t \\big(1   - \\frac{L}{2} t\\big) \\|∇f (x)\\|^2_2, \\\\\n",
    "$$\n",
    "\n",
    "- (b) Use $t ≤ 1/L$, and rearrange the previous result, to \n",
    "$$\n",
    "‖∇f (x)‖_2^2 ≤ \\frac{2}{t} (f (x) − f (x+)) .\n",
    "$$\n",
    "\n",
    "Using the result from a...\n",
    "\n",
    "$$\n",
    "f(x^+) \\leq f (x) - t \\big(1   - \\frac{L}{2} t\\big) \\|∇f (x)\\|^2_2, \\\\\n",
    "f(x^+) - f (x)\\leq - t \\big(1   - \\frac{L}{2} t\\big) \\|∇f (x)\\|^2_2, \\\\\n",
    "f (x) - f(x^+) \\geq  t \\big(1   - \\frac{L}{2} t\\big) \\|∇f (x)\\|^2_2, \\\\\n",
    "\\frac{f (x) - f(x^+)}{t \\big(1   - \\frac{L}{2} t\\big)}\\geq \\|∇f (x)\\|^2_2, \\\\\n",
    "\\frac{f (x) - f(x^+)}{ \\frac{1}{L}\\big(1   - \\frac{L}{2} \\frac{1}{L}\\big)} \\geq \\frac{f (x) - f(x^+)}{t \\big(1   - \\frac{L}{2} t\\big)}\\geq \\|∇f (x)\\|^2_2, \\\\\n",
    "\\frac{f (x) - f(x^+)}{t \\big(1   - \\frac{1}{2} \\big)} \\geq \\|∇f (x)\\|^2_2, \\\\\n",
    "f (x) - f(x^+)\\frac{2}{t} \\geq \\|∇f (x)\\|^2_2, \\\\\n",
    "$$\n",
    "\n",
    "- (c) Sum the previous result over all iterations from $1, . . . , k + 1$ to establish\n",
    "$$\n",
    "\\sum_{i = 0}^k \\|∇f (x^{(i))}\\|_2^2 \\leq \\frac{2}{t}(f (x^{(0)}) − f^*) ,\n",
    "$$\n",
    "where $f^*$ is the minimal value of $f$.\n",
    "\n",
    "We start with summing the previous inequality:\n",
    "$$\n",
    "\\sum_{i = 0}^k \\|∇f (x^{(i))}\\|_2^2 \\leq \\sum_{i = 0}^k \\frac{2}{t}f (x^{(i)})- f(x^{(i+1)}) \\\\\n",
    "\\sum_{i = 0}^k \\|∇f (x^{(i))}\\|_2^2 \\leq \\frac{2}{t}\\sum_{i = 0}^k f (x^{(i)})- f(x^{(i+1)}) \\\\\n",
    "\\text{The right side is a telescoping sum, resulting in:} \\\\\n",
    "\\sum_{i = 0}^k \\|∇f (x^{(i))}\\|_2^2 \\leq \\frac{2}{t} \\big(  f(x^{(0)})- f(x^{(k+1)}) \\big)\\\\\n",
    "\\text{Since we know $f^* \\leq f(x^{(k+1)})$, we have} \\\\\n",
    "\\sum_{i = 0}^k \\|∇f (x^{(i))}\\|_2^2 \\leq \\frac{2}{t} \\big( f(x^{(0)})- f(x^{(k+1)}) \\big) \\leq \\frac{2}{t}  \\big(f(x^{(0)})- f^* \\big)\\\\\n",
    "\\sum_{i = 0}^k \\|∇f (x^{(i))}\\|_2^2  \\leq \\frac{2}{t}  \\big(f(x^{(0)})- f^* \\big)\\\\\n",
    "$$\n",
    "\n",
    "- (d) Lower bound the sum in the previous result to get\n",
    "$$\n",
    "min_{i=0,...,k} \\|∇f (x^{(i)})\\|_2 \\leq \\sqrt{\\frac{2}{t(k + 1)} (f(x^{(0)}) − f^*)},\n",
    "$$\n",
    "which establishes the desired $O (1/\\epsilon^2)$ rate for achieving $\\epsilon$-substationarity.\n",
    "\n",
    "We consider the average squared gradient:\n",
    "\n",
    "$$\n",
    "\\frac{1}{k+1} \\sum_{i = 0}^k \\|∇f (x^{(i))}\\|_2^2  \\leq \\frac{1}{k+1} \\frac{2}{t}  \\big(f(x^{(0)})- f^* \\big)\\\\\n",
    "\\text{We know that for averages:} \\\\\n",
    "min_{i=0,...,k} \\|∇f (x^{(i)})\\|_2^2 \\leq \\frac{1}{k+1} \\sum_{i = 0}^k \\|∇f (x^{(i))}\\|_2^2 \\\\\n",
    "min_{i=0,...,k} \\|∇f (x^{(i)})\\|_2^2 \\leq \\frac{1}{k+1} \\frac{2}{t}  \\big(f(x^{(0)})- f^* \\big)\\\\\n",
    "min_{i=0,...,k} \\|∇f (x^{(i)})\\|_2 \\leq \\sqrt{\\frac{2}{t (k+1)}  \\big(f(x^{(0)})- f^* \\big)}\\\\\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
