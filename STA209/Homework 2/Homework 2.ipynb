{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2\n",
    "\n",
    "## Gianni Spiga\n",
    "\n",
    "### For STA 209, Fall 2023\n",
    "\n",
    "- I discussed approaches and methodology to these problems with Jonas Kempf and Niraj Bangari "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Determine whether the following sets or functions are convex or not:\n",
    "\n",
    "- a. $\\{x  \\in \\mathbb{R}^n : Ax = b\\}, \\text{where}  \\ A \\in \\mathbb{R}^{m x n}, b \\in \\mathbb{R}^m$\n",
    "\n",
    "We let $Ax = b, Ay = b$:\n",
    "$$\n",
    "\\begin{align*}\n",
    "A(\\alpha x + (1 - \\alpha) y) &= \\alpha A x + (1 - \\alpha) A y \\\\ \n",
    "&= (\\alpha + 1 - \\alpha)b \\\\\n",
    "&= b\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Thus the set is complex.\n",
    "\n",
    "- b.  $\\{x  \\in \\mathbb{R}^n : ||x - x_0||_2 = r\\}, \\text{where} \\ x_0 \\in \\mathbb{R}^{n}, r \\in \\mathbb{R}$\n",
    "\n",
    "We take two vectors, $y_1, y_2 \\in \\mathbb{R}^{n}$ such that $x = \\alpha y_1 + (1 - \\alpha) y_2$ as well as $||y_1 - x_0||_2 = r, ||y_2 - x_0||_2 = r$. We have:\n",
    "$$\n",
    "\\begin{align*}\n",
    "||x - x_0||_2 &= ||\\alpha y_1 + (1 - \\alpha) y_2 - x_0||_2 \\\\ \n",
    "& \\text{From Triangle Inequality:} \\\\\n",
    "||\\alpha y_1 + (1 - \\alpha) y_2 - x_0||_2 &\\leq \\alpha||y_1 - x_0|| + (1 - \\alpha)||y_2 - x_0|| \\\\\n",
    "&\\leq \\alpha r + (1 - \\alpha)r\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Thus our vector $x$ is in a convex set. \n",
    "\n",
    "- c. $f(x_1, x_2) = (x_1 x_2 - 1)^2 \\text{where} \\ x_1, x_2 \\in \\mathbb{R}$\n",
    "\n",
    "We know $f$ is convex iff the $\\nabla^2 f(x)$ is positive semi-definite. We first find first and second order derivatives:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial f}{\\partial x_1} &= 2(x_1 x_2 - 1)x_2 = 2x_1 x_2^2 - 2x_2 \\\\\n",
    "\\frac{\\partial f}{\\partial x_2} &= 2(x_1 x_2 - 1)x_1 = 2x_1^2 x_2 - 2x_1\\\\\n",
    "\\frac{\\partial^2 f}{\\partial x_1^2} &= 2x_2^2 \\\\\n",
    "\\frac{\\partial^2 f}{\\partial x_2^2} &= 2x_1^2 \\\\\n",
    "\\frac{\\partial^2 f}{\\partial x_1x_2} &= 4 x_1 x_2 - 2 \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "H &= \\begin{bmatrix}\n",
    "2x_2^2 & 4x_1x_2 - 2\\\\\n",
    "4x_1x_2 - 2 & 2x_1^2\n",
    "\\end{bmatrix}  \\\\\n",
    "\n",
    "\\det(H) &= (2 x_2^2)(2 x_1^2) - (4x_1 x_2 - 2)(4 x_1 x_2 - 2) \\\\\n",
    "&= 4 x_2^2 x_1^2 - (4x_1x_2 - 2)^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Since the above determinant is negative at points like $(0,0)$, the function $f$ is not convex. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Compute the gradient $\\nabla f$ and Hessian $\\nabla^2 f$ of the Rosenbrock function $f : \\mathbb{R}^2 \\rightarrow  \\mathbb{R}$\n",
    "defined by\n",
    "$$\n",
    "f (x) = 100(x_2 − x^2_1)^2 + (1 − x_1)^2\n",
    "$$.\n",
    "\n",
    "Show that $x^∗ = ( 1, 1 )^T$ is the only local minimizer of this function, and that the Hessian matrix at that point is positive definite.\n",
    "\n",
    "Find derivatives:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial f}{\\partial x_1} &=  100 * 2 * (x_2 − x^2_1) * -2x_1 - 2(1 - x_1) = -400(x_1x_2 - x^3_1)- 2(1 - x_1)\\\\\n",
    "\\frac{\\partial f}{\\partial x_2} &= 100 * 2 * (x_2 − x^2_1) = 200 (x_2 − x^2_1)\\\\\n",
    "\\frac{\\partial^2 f}{\\partial x_1^2} &= -400x_2 + 1200 x^2_1 +  2 \\\\\n",
    "\\frac{\\partial^2 f}{\\partial x_2^2} &= 200 \\\\\n",
    "\\frac{\\partial^2 f}{\\partial x_1x_2} &= -400x_1 \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "First we need to show the $( 1, 1 )^T$ is the only local minimizer of this function. We solve:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} &= \\begin{bmatrix}\n",
    "-400(x_1x_2 - x^3_1)- 2(1 - x_1)\\\\\n",
    "200 (x_2 − x^2_1)\n",
    "\\end{bmatrix} \\\\\n",
    "&\\text{We start with the second equation:} \\\\\n",
    "0 &= 200x_2 - 200x^2_1 \\\\\n",
    "\n",
    "200x_2 &= 200x_1^2 \\\\\n",
    "x_2 &= x_1^2 \\\\\n",
    "\n",
    "&\\text{Now we replace $x_2$ with $x_1^2$ in first equation} \\\\\n",
    "0 &= 400(x_1^3 - x^3_1)- 2(1 - x_1) \\\\\n",
    "&= -2 - 2x_1 \\\\\n",
    "1 &= x_1 \\rightarrow x_2 = 1 \\\\\n",
    "\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Thus the point $( 1, 1 )^T$ is the only local minimizer. We evaluate the Hessian at the point $( 1, 1 )^T$:\n",
    "$$\n",
    "\\begin{align*}\n",
    "H &= \\begin{bmatrix}\n",
    "-400x_2 + 1200 x^2_1 +  2 & -400x_1\\\\\n",
    "-400x_1 & 200\n",
    "\\end{bmatrix}  \\\\\n",
    "&= \\begin{bmatrix}\n",
    "-400*1 + 1200 *1 +  2 & -400 * 1\\\\\n",
    "-400 * 1 & 200\n",
    "\\end{bmatrix}  \\\\\n",
    "\n",
    "&= \\begin{bmatrix}\n",
    "802 & -400 \\\\\n",
    "-400 & 200\n",
    "\\end{bmatrix}  \\\\\n",
    "\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We now find the eigenvalues of the following matrix\n",
    "$$\n",
    "\\begin{align*}\n",
    "\n",
    " &= \\begin{bmatrix}\n",
    "802 - \\lambda & -400 \\\\\n",
    "-400 & 200 - \\lambda\n",
    "\\end{bmatrix}  \\\\\n",
    "\n",
    "&\\rightarrow (802 - \\lambda)(200 - \\lambda) - 400^2 \\\\\n",
    "0 &= 400 - 802\\lambda - 200\\lambda + \\lambda^2 \\\\\n",
    "&= 400 - 1002\\lambda+ \\lambda^2 \\\\\n",
    "\n",
    "\\text{We use quadratic formula:} \\\\\n",
    "\\lambda &= \\frac{1002 \\pm \\sqrt{(-1002)^2 - 4*400}}{2} \\\\\n",
    "\\lambda &= \\frac{1002 \\pm 1001.20}{2} > 0\n",
    "\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Since our eigenvalues are strictly greater than 0, we have a positive definite Hessian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Let $X$ be a real-valued random variable which takes values in $\\{a_1, \\ldots , a_n \\}$, where $a_1 <\n",
    "a_22 < \\ldots < a_n$ with $P(X = a_i) = p_i, i = 1, \\ldots , n$. The probability simplex is a set defined\n",
    "by $\\{ p \\in \\mathbb{R}^n_{++} | \\bold{1}^Tp = 1 \\}$, where $\\mathbb{R}_{++}$ denotes the set of positive real numbers and $\\bold{1}$\n",
    "denotes the vector with all entries equal to one.\n",
    " - (a) Show that the probability simplex is a convex set.\n",
    "\n",
    " We can think of two probability vectors, call them $\\psi$ and $\\rho$. We can put it into the following form to check for convexity, keeping in mind that multiplying by a vector of 1's is the same as summing the elements of the vector. \n",
    "\n",
    " $$\n",
    "\\begin{align*}\n",
    "\\alpha \\psi + (1 - \\alpha) \\rho &= \\sum_{i=1}^n \\alpha \\psi_i + (1 - \\alpha) \\rho_i \\\\\n",
    "&=  \\alpha \\sum_{i=1}^n\\psi_i + (1 - \\alpha) \\sum_{i=1}^n\\rho_i \\\\\n",
    "&=  \\alpha * 1 + (1 - \\alpha) * 1 \\\\\n",
    "&= 1\n",
    "\\end{align*}\n",
    " $$\n",
    "\n",
    " Thus  \\alpha \\psi + (1 - \\alpha) \\rho is a probability vector and the simplex is a convex set. \n",
    "\n",
    "\n",
    " - (b) The entropy function defined on the probability simplex is given by\n",
    "$$\n",
    "f(p) = −\\sum_{i =1}^n p_i \\log p_i.\n",
    "$$\n",
    "Show that the entropy function is strictly concave.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial f}{\\partial p_i} &= - \\log p_i - 1 \\\\\n",
    "\\frac{\\partial^2 f}{\\partial p_i^2} &= - \\frac{1}{p_i}  \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "All partial derivatives with respect to $p_ip_j, i\\neq j$ will be equal to 0. Thus our Hessian is a diagonal matrix of $- \\frac{1}{p_i}$, which means the eigenvalues for this matrix are $- \\frac{1}{p_i}$, thus the matrix is not positive definite or semi-definite (negative definite), meaning the entropy function is concave. \n",
    "\n",
    "- (c) Show that the variance\n",
    "$$\n",
    " f(p) = Var(X) = E\\big(X − E (X)\\big)^2\n",
    "$$\n",
    "as a function of $p$ defined on the probability simplex is a concave function.\n",
    "\n",
    "We can also rewrite the function in the following form:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "f(p) &= Var(X) = E(X^2) − \\big(E(X)\\big)^2 \\\\ \n",
    "&= \\sum_{i =1}^n p_i a_i^2 + \\big(\\sum_{i =1}^n p_i a_i \\big)^2 \\\\\n",
    "\\frac{\\partial f}{\\partial p_i} &= a_i^2 - 2 a_i (\\sum_{j =1}^n p_j a_j \\big) \\\\ \n",
    "\\frac{\\partial^2 f}{\\partial p_i^2} &= - 2 a_i a_j \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Thus the Hessian is $- 2 a_i a_j$ when $i = j$ and $0$ otherwise. Since this matrix is a diagonal matrix with all negative values, the eigenvalues will all be negative. Thus, the Hessian is negative definite and the function is concave.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Identify the stationary points of $f(x) = 8x_1 + 12x_2 + x^2_1 − 2x^2_2$. Are they local minimum\n",
    "or maximum, global minimum or maximum or saddle points? Why?\n",
    "\n",
    "First we find the derivatives. \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial f}{\\partial x_1} &= 8 + 2x_1 \\\\\n",
    "\\frac{\\partial f}{\\partial x_2} &= 12 - 4x_2 \\\\\n",
    "\\frac{\\partial^2 f}{\\partial x_1^2} &= 2 \\\\\n",
    "\\frac{\\partial^2 f}{\\partial x_2^2} &= -4 \\\\\n",
    "\\frac{\\partial^2 f}{\\partial x_1 x_2} &= 0 \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Use first derivatives for stationary points. \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "0 &= 8 + 2x_1 \\\\\n",
    "-8 &- 2x_1 \\\\\n",
    "-4 &= x_1 \\\\\n",
    "0 &= 12 - 4x_2 \\\\\n",
    "-12 &= - 4x_2 \\\\\n",
    "3 &= x_2 \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "To identify the stationary points in relation to what kind of minimum they are, we must look at the Hessian.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "H &= \\begin{bmatrix}\n",
    " 2 & 0\\\\\n",
    "0 & -4\n",
    "\\end{bmatrix}  \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Since this Hessian has both a negative and a positive Eigenvalue, the point $x = (x_1, x_2) = (-4, 3)$ is a saddle point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. The maximum likelihood estimator of the binary logistic regression is defined as\n",
    "$$\n",
    "\\arg \\min_{\\omega \\in \\mathbb{R}^d} f(\\omega),\n",
    "$$\n",
    "where $f : \\mathbb{R}^d → \\mathbb{R}$ is given by\n",
    "$$\n",
    "f(\\omega) = \\sum_{i =1}^n \\log(1 + \\exp\\{−y_i \\omega^T x_i\\})\n",
    "$$\n",
    "for $\\omega \\in \\mathbb{R}^d, x_1, \\ldots , x_n \\in \\mathbb{R}^d \\ \\text{and} \\ y_1, \\ldots , y_n \\in \\{−1, 1\\}$.\n",
    "\n",
    "-(a) Compute the gradient $\\nabla f$ and the Hessian $\\nabla^2 f$ as function of the variable $w$ (for\n",
    "that we use the notation $\\nabla_w f$ and $\\nabla^2_w f$ ).\n",
    "\n",
    "The gradient $\\nabla_w f$ will be the following $ \\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial \\omega_1} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial f}{\\partial \\omega_d}\n",
    "\\end{bmatrix}$, we first find first-order partial derivatives for each $\\omega_j$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial f}{\\partial \\omega_j} &= \\frac{1}{1 + \\exp\\{−y_i \\omega^T x_i\\}} * \\frac{\\partial }{\\partial \\omega} \\big(1 + \\exp\\{−y_i \\omega^T x_i\\} \\big) \\\\\n",
    "& = \\frac{- y_i x_i\\exp\\{−y_i \\omega^T x_i\\}}{1 + \\exp\\{−y_i \\omega^T x_i\\}} \\\\\n",
    "\\nabla_w f &= \\begin{bmatrix}\n",
    "\\frac{- y_i x_i\\exp\\{−y_i \\omega_1 x_i\\}}{1 + \\exp\\{−y_i \\omega_1 x_i\\}}  \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{- y_i x_i\\exp\\{−y_i \\omega_d x_i\\}}{1 + \\exp\\{−y_i \\omega_d x_i\\}} \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "To find the Hessian, we will need 2nd order derivatives:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial^2 f}{\\partial \\omega_j \\omega_k} &= \\frac{\\partial }{\\partial \\omega} \\frac{- y_i x_i\\exp\\{−y_i \\omega_d x_i\\}} {1 + \\exp\\{−y_i \\omega_d x_i\\}} \\\\ \n",
    "&\\text{Let } \\lambda = \\exp\\{−y_i \\omega_d x_i\\} \\\\ \n",
    "&= \\frac{\\big(1 + \\lambda \\big) * \\frac{\\partial}{\\partial \\omega}- y_i x_i\\lambda - \\frac{\\partial }{\\partial \\omega}  \\big(1 + \\lambda\\big) * - y_i x_i\\lambda}{\\big(1 + \\lambda\\big)^2} \\\\\n",
    "&= \\frac{\\big(1 + \\lambda \\big) *  y_i^2 x_{i,j} x_{i,k}\\lambda -   \\big(- y_i x_i\\lambda) * - y_i x_i\\lambda}{\\big(1 + \\lambda\\big)^2} \\\\\n",
    "&= \\frac{\\big(y_i^2 x_{i,j} x_{i,k}\\lambda + y_i^2 x_{i,j} x_{i,k}\\lambda^2 \\big)  -  y_i^2 x_{i,j} x_{i,k}\\lambda^2}{\\big(1 + \\lambda\\big)^2} \\\\\n",
    "&= \\frac{y_i^2 x_{i,j} x_{i,k}\\lambda}{\\big(1 + \\lambda\\big)^2} \\\\\n",
    "&\\text{Substituting back in:} \\\\\n",
    "&= \\frac{y_i^2 x_{i,j} x_{i,k}\\exp\\{−y_i \\omega_d x_i\\}}{\\big(1 + \\exp\\{−y_i \\omega_d x_i\\}\\big)^2} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- (b) Compute the First and second order Taylor expansion of the function $f$ around the\n",
    "point $w_0 = 0$\n",
    "\n",
    "For the first order Taylor expansion: \n",
    "$$\n",
    "\\begin{align*}\n",
    "f(\\omega) &\\approx f(\\omega_0) + (w - w_0) \\nabla_w f \\\\\n",
    "f(\\omega) &\\approx f(0) + (w - 0)^T \\nabla_w f(0) \\\\\n",
    "f(\\omega) &\\approx f(\\omega) = \\sum_{i =1}^n \\log(1 + \\exp\\{−y_i 0^T x_i\\}) + (w)^T \\frac{- y_i x_i\\exp\\{−y_i 0^T x_i\\}}{1 + \\exp\\{−y_i 0^T x_i\\}}\\\\\n",
    "f(\\omega) &\\approx f(\\omega) = n \\log(2) + w^T \\frac{- y_i x_i}{2}\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "For the second-order Taylor Expansion:\n",
    "$$\n",
    "\\begin{align*}\n",
    "f(\\omega) &\\approx f(0) + (w - 0)^T \\nabla_w f(0) + \\frac{1}{2}\\omega^T  \\nabla^2 f(0) \\omega \\\\\n",
    "f(\\omega) &\\approx f(\\omega) = \\sum_{i =1}^n \\log(1 + \\exp\\{−y_i 0^T x_i\\}) + (w)^T \\frac{- y_i x_i\\exp\\{−y_i 0^T x_i\\}}{1 + \\exp\\{−y_i 0^T x_i\\}} + \\frac{1}{2}\\omega^T \\frac{y_i^2 x_{i,j} x_{i,k}\\exp\\{−y_i 0^T x_i\\}}{\\big(1 + \\exp\\{−y_i 0^T x_i\\}\\big)^2} \\omega\\\\\n",
    "f(\\omega) &\\approx f(\\omega) = n \\log(2) + w^T \\frac{- y_i x_i}{2} + \\frac{1}{8}\\omega^T y_i^2 x_{i,j} x_{i,k} \\omega\\\\\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- (c) Compute the First and second order Taylor expansion of the function $f$ around the\n",
    "point $w_0 = [1, \\ldots, 1]^T$.\n",
    "\n",
    "For the first order approximation:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "f(\\omega) &\\approx \\sum_{i =1}^n \\log(1 + \\exp\\{−y_i 1^T x_i\\}) + (w - [1, \\ldots, 1]^T)^T \\frac{- y_i x_i\\exp\\{−y_i 1^T x_i\\}}{1 + \\exp\\{−y_i 1^T x_i\\}} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "For the second order approximation:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "f(\\omega) &\\approx \\sum_{i =1}^n \\log(1 + \\exp\\{−y_i 1^T x_i\\}) + (w - [1, \\ldots, 1]^T)^T \\frac{- y_i x_i\\exp\\{−y_i 1^T x_i\\}}{1 + \\exp\\{−y_i 1^T x_i\\}}  + \\frac{1}{2}(\\omega - [1, \\ldots, 1]^T)^T \\frac{y_i^2 x_{i,j} x_{i,k}\\exp\\{−y_i 1^T x_i\\}}{\\big(1 + \\exp\\{−y_i 1^T x_i\\}\\big)^2} (\\omega - [1, \\ldots, 1]^T)\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Consider the function $f(x) = \\log x + e^x + \\frac{1}{x}$\n",
    "\n",
    "- (a) Compute the derivative of $f$. What term dominates in the expression when $x$ is close\n",
    "to zero?\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{df}{d x} = \\frac{1}{x} + e^x - \\frac{1}{x^2} \\\\ \n",
    "\\frac{d^2f}{dx^2} = - \\frac{1}{x^2} + e^x + \\frac{2}{x^3}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The term that dominates in the expression would be $- \\frac{1}{x^2}$ since it grows in such larger magnitude than the other terms. \n",
    "\n",
    "- (b) Construct function $g_{x_0}(x)$ to be the first order Taylor approximation of $f$ around\n",
    "$x_0 = 1$. Construct function $h_{x_0}(x)$ to be the second order Taylor approximation of $f$\n",
    "around $x_0 = 1$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "g_{x_0}(x) &= f(x_0) + f'(x_0)(x-x_0) \\\\\n",
    "&= f(1) + f'(1)(x-1) \\\\\n",
    "&= (log(1) + e + 1) + (1+e -1)(x-1) \\\\ \n",
    "&= e + 1 + e(x-1) \\\\ \n",
    "&= e + 1 + ex \\\\ \n",
    "h_{x_0}(x) &= g_{x_0}(x) + \\frac{1}{2}f''(x_0)(x - x_0)^2 \\\\\n",
    "&= ex + e + 1 + \\frac{1}{2}(-1 + e + 2)(x - 1)^2 \\\\\n",
    "&= ex + e + 1 + \\frac{1}{2} (e+1) (x - 1)^2 \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- (c) Evaluate the functions $f , g_{x_0}$ and $h_{x_0}$ at point $x = 2$. Compute the approximation\n",
    "error for each.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "g_{x_0}(2) &= e + 1 + 2e  \\\\\n",
    "&= 1 + 3e  \\\\\n",
    "h_{x_0}(2) &= 2e + e + 1 + \\frac{1}{2} (e+1) (2 - 1)^2\\\\\n",
    "&= 3\\frac{1}{2}e + \\frac{3}{2}\\\\\n",
    "\n",
    "\\epsilon_{1st} &= |\\big(log(2) + e^2 - \\frac{1}{2}+ 3e)| \\\\\n",
    "\n",
    "\\epsilon_{2nd} &= |\\log 2 + e^2 + \\frac{1}{2} -  (3\\frac{1}{2}e + \\frac{3}{2})| \\\\ \n",
    "&= | |\\log 2 + e^2 - \\frac{1}{2} -  3\\frac{1}{2}e| \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (d) Construct function $g_{x_1} (x)$ to be the first order Taylor approximation of f around\n",
    "$x_1 = 3$. Construct function $h_{x_1} (x)$ to be the second order Taylor approximation of $f$\n",
    "around $x_1 = 3$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "g_{x_1}(x) &= f(x_0) + f'(x_0)(x-x_0) \\\\\n",
    "&= f(3) + f'(3)(x-1) \\\\\n",
    "&= (log(3) + e^3 + \\frac{1}{3}) + (\\frac{1}{3} + e^3 - \\frac{1}{9})(x-1) \\\\ \n",
    "\n",
    "h_{x_1}(x) &= (log(3) + e^3 + \\frac{1}{3}) + (\\frac{1}{3} + e^3 - \\frac{1}{9})(x-1) + \\frac{1}{2} (-\\frac{1}{9} + e^3 + \\frac{2}{27})(x - 3)^2 \\\\\n",
    "\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- (e) Evaluate the functions $f$ , $g_{x_1}$ and $h_{x_1}$ at point $x = 2$. Compute the approximation\n",
    "error for each.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "g_{x_1}(x) &= (log(3) + e^3 + \\frac{1}{3}) + (\\frac{1}{3} + e^3 - \\frac{1}{9})(2-1) \\\\ \n",
    "&= (log(3) + 2e^3 + \\frac{5}{9})\\\\ \n",
    "\n",
    "h_{x_1}(x) &= (log(3) + e^3 + \\frac{1}{3}) + (\\frac{1}{3} + e^3 - \\frac{1}{9})(2-1) + \\frac{1}{2} (-\\frac{1}{9} + e^3 + \\frac{2}{27})(2 - 3)^2 \\\\\n",
    "&=  (log(3) + 3e^3 + \\frac{5}{9}) + \\frac{1}{2} (-\\frac{1}{9}) \\\\ \n",
    "&=  (log(3) + 3e^3 + \\frac{5}{9}) - \\frac{1}{18} \\\\ \n",
    "&=  (log(3) + 3e^3 + \\frac{1}{2}) \\\\ \n",
    "\\epsilon_{1st} &= |f(2) - g(2)| \\\\\n",
    "&= |\\big(log(2) + e^2 + \\frac{1}{2}) - (log(3) + 2e^3 + \\frac{5}{9})| \\\\\n",
    "&= |\\big(log(2) + e^2 - \\frac{1}{18}) - (log(3) + 2e^3)| \\\\\n",
    "\\epsilon_{2nd} &= |\\big(log(2) + e^2) - (log(3) + 3e^3)| \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
